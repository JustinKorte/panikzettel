\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hanging}
\usepackage{calc}
\usepackage{hyperref}
\usepackage{bookmark}
\usepackage[ngerman]{babel}
\title{Lineare Algebra Panikzettel}
\author{Philipp Schröer, Tobias Polock, Caspar Zecha}
\date{\today}

% Formatting
\setlength\parindent{0pt}
\relpenalty=9999
\binoppenalty=9999

% Definitions
\newlength{\hangwidth}
\newcommand{\skript}[1]{\settowidth{\hangwidth}{\textbf{(#1)} }\hangpara{\hangwidth}{1}\textbf{(#1)} }% Für Referenzen auf das Skript. Benutzung: \skript{Stelle}Inhalt

\newcommand{\id}{\mathrm{id}}%Die Identitätsfunktion
\newcommand{\Sol}{\mathrm{Sol}}%Lösungsraum
\newcommand{\Col}{\mathrm{Col}}%Spaltenraum
\newcommand{\rk}{\mathrm{rk}}%Rang
\newcommand{\defekt}{\mathrm{def}}%Defekt
\newcommand{\GL}{\mathrm{GL}}%Lineare Gruppe
\newcommand{\Ker}{\mathrm{Ker}}%Kern
\renewcommand{\Im}{\mathrm{Im}}%Bild
\newcommand{\Field}{\mathbb{F}}%Endlicher Körper
\newcommand{\Eig}{\mathrm{Eig}}%Eigenraum
\newcommand{\geo}{\mathrm{g}}%Geometrische Vielfachheit
\newcommand{\mul}{\mathrm{m}}%Algebraische Vielfachheit
\newcommand{\pr}{\mathrm{pr}}%Projektion

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%Betrag
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%Norm


\begin{document}
\maketitle

\setcounter{tocdepth}{2}
\tableofcontents

\section{Einleitung}

Dieser ``Panikzettel'' für Lineare Algebra ist eine mehr oder weniger informelle Zusammenfassung des Skriptes von \href{http://www.math.rwth-aachen.de/~Sebastian.Thomas/photo.jpg}{Dr. Sebastian Thomas} und der Vorlesung Lineare Algebra im SS 2016 bei PD Dr. Viktor Levandovskyy. Es werden hier die wichtigsten Aussagen, Tipps und Erklärungen gesammelt, die hoffentlich in dem unvermeidbaren Moment der Panik bei Hausaufgaben oder beim Lernen helfen. Wir haben etwas Formalismus im Sinne der Übersichtlichkeit weggelassen, wie etwa offensichtliche $n \in \mathbb{N}$.

Dieses Projekt ist lizenziert unter \href{https://creativecommons.org/licenses/by-sa/4.0/}{CC-BY-SA-4.0} und wird auf dem Git-Server der RWTH verwaltet: \url{https://git.rwth-aachen.de/philipp.schroer/panikzettel}

\section{Vektorräume und ihre Homomorphismen}

$K$-Vektorraum $V$ hat die Operationen Addition  $V \times V \to V, (v,w) \mapsto v+w$, sowie die Skalarmultiplikation $K \times V \to V, (a,v) \mapsto a \cdot v  $, für den folgende Axiome mit $v, w, x \in V, a,b \in K$ gelten müssen:

\begin{enumerate}
\item Assoziativität d. Addition: $(v+w)+x = v+(w+x)$
\item Nullvektor: $\exists 0 \in V: v+0=0+v=v$
\item Negative: $(-v)+v=v+(-v)=0$
\item Kommutative Additon: $v+w=w+v$
\item Assoziative Skalarmultiplikation: $a(bv)=(ab)v$
\item Einselement: $1\cdot v=v$
\item Distributivität: $(a+b)v=(av)+(bv)$, sowie $a(v+w)=(av)+(aw)$
\end{enumerate}

\subsection{Untervektorräume}

\skript{1.10} Für $K$-Vektorräume $U$ und $V$ heißt $U$ Untervektorraum von $V$ wenn die Menge der Vektoren von $U$ Teilmenge der Menge der Vektoren von $V$ ist und für $k \in K, u,u^\prime \in U$ gilt:

\begin{align*}
	u {+}^{U} u^\prime &= u {+}^{V} u^\prime	\\
	k \cdot u^\prime &= k \cdot u^\prime
\end{align*}

Wir schreiben $U \leq V$ für ``$U$ ist ein Untervektorraum von $V$'' und $U \nleq V$ für ``$U$ ist kein Untervektorraum von $V$''.

\subsection{Linearkombinationen und Lineare Unabhängigkeit}

Eine linearkombination eines Tupels $s = (s_1,s_2, \ldots, s_n)$ in einem $K$-Vektorraum $V$ ist eine Summe der Einträge von $s$ mit Vorfaktoren aus $K$.	\\
Ein Tupel heißt linear unabhängig, wenn kein Eintrag eine Linearkombination der anderen Einträge ist.

\subsection{Erzeugendensysteme, Basen und Dimension}

Für einen $K$-Vektorraum $V$ heißt ein Tupel $s$ in $V$ Erzeugendensystem von $V$ wenn jedes Element von $V$ eine Linearkombination von $s$ ist.	\\
Für ein Tupel $s$ in $V$ bezeichnet $\langle s \rangle$ die Menge aller Linearkombinationen von $s$ und wird Erzeugnis von $s$ genannt. Das Erzeugnis eines belibigen Tupels in einem $K$-Vektorraum $V$ ist ein Untervektorraum von $V$.	\\
Ein linear unabhängiges Erzeugendensystem wird Basis genannt. Gibt es eine endliche Basis von $V$ der länge $n$, dann nennen wir $V$ endlich erzeugt und sagen dass $V$ Dimension $n$ hat.

\subsection{Unabhängigkeit von Untervektorräumen und innere direkte Summe}

\skript{1.65} Für einen $K$-Vektorraum $V$ und Untervektorräume $U_1,\ldots,U_n$ von $V$ ist die innere Summe von $(U_1,\ldots,U_n)$:
$$ U_1 + \ldots + U_n =  \sum_{i = 1}^n U_i := \left\{ \sum_{i=1}^n u_i \vert u_i \in U_i \right\}$$
Sind für jedes $u \in U_1 + \ldots + U_n$ die $u_i \in U_i$ mit $u = \sum_{i=1}^n u_i$ eindeutig gegeben, heißt $(U_1,\ldots,U_n)$ unabhängig. In diesem fall nennen wir die innere Summe auch innere direkte Summe, geschrieben
$$ U_1 \dot+ \ldots \dot+ U_n = \dot\sum_{i=1}^n U_i := \sum_{i=1}^n U_i$$

\skript{1.68} Für $n$-Tupel $s = (s_1,\ldots,s_n)$ in $V$ gilt:
\begin{enumerate}
	\item $\langle s_1,\ldots,s_n\rangle = \sum_{i=1}{n} Ks_i$
	\item $s$ ist Erzeugendensystem von V genau dann wenn $V = \sum_{i=1}^n Ks_i$ ist.
	\item $s$ ist linear Unabhängig genau dann wenn alle $s_i \neq 0$ sind und $(Ks_1,\ldots,Ks_n)$ unabhängig ist.
	\item $s$ ist eine Basis von V genau dann wenn $V = \dot\sum_{i=1}^n Ks_i$ ist.
\end{enumerate}

\skript{1.70} Ein Tupel $(U_1,\ldots,U_n)$ von Untervektorräumen von $V$ ist unabhängig genau dann wenn für alle $i \in \mathbb{N}, i \le n$ gilt: $U_i \cap \sum_{j \in \{1,\ldots,n\}\setminus\{i\}} U_j = \{0\}$

Insbesondere sind zwei Untervektorräume $U_1$ und $U_2$ unabhängig genau dann wenn $U_1 \cap U_2 = \{0\}$ ist.

\skript{1.74} Für einen $K$-Vektorraum $V$ und Untervektorräume $(U_1,\ldots,U_n)$ mit $V = \dot\sum_{i=1}^n U_i$ ist die Projektion von $V$ auf $U_i$ definiert als
$$ \pr_i: V \to U_i, \left(\sum_{i=1}^n u_i\right) \mapsto u_i \text{ für } u_i \in U_i$$

\skript{1.76} Für einen Untervektorraum $U$ von $V$ heißt $U^\prime$ Komplement von $U$ wenn $$V = U \dot+ U^\prime$$

\subsection{Homomorphismen}

Ein Vektorraumhomomorphismus ist eine Funktion $\varphi : V \to W$ zwischen zwei Vektorräumen, welcher folgende Axiome für $K$-Vektorräume $V$ und $W$ erfüllt:

\skript{2.2b} \begin{enumerate}
	\item Additivität: Für $v, v^\prime \in V$ ist $\varphi(v + v^\prime) = \varphi(v) + \varphi(v^\prime)$
	\item Homogenität: Für $a \in K, v \in V$ ist $\varphi(av) = a \varphi(v)$
\end{enumerate}

Oder direkt in einem Axiom:

\skript{2.2c} Für $a \in K, v, v^\prime \in V$ ist $\varphi(av + v^\prime) = a \varphi(v) + \varphi(v^\prime)$.\\

\skript{2.5} Ein Vektorraum\underline{endo}morphismus ist ein Vektorraum\underline{homo}morphismus von $V$ nach $V$.

\skript{2.9} Ein Vektorraum\underline{iso}morphismus ist ein bijektiver/invertierbarer Vektorraum\underline{homo}morphismus.

\skript{2.10} Ein Vektorraum\underline{auto}morphismus ist ein Vektorraum\underline{iso}morphismus von $V$ nach $V$. Andere Namen: $\textrm{GL}(V) = \textrm{Aut}(V) = \textrm{Aut}_K(V) := \textrm{End}_K(V)^\times$

\subsubsection{Bild und Kern, Rang und Defekt}

Es seien $K$-Vektorräume $V$ und $W$ und ein $K$-Vektorraumhomomorphismus $\varphi: V \to W$

\skript{2.11 a)} $\Im \varphi$ ist ein $K$-Untervektorraum von $W$

\skript{2.11 b)} $\Ker \varphi \coloneqq \{v \in V | \varphi(v)=0 \}$ ist ein $K$-Untervektorraum von $V$

\skript{2.21 a)} $\defekt_K(\varphi) \coloneqq \dim_K(\Ker\varphi)$

\skript{2.21 b)} $\rk_K(\varphi) \coloneqq \dim_K(\Im\varphi)$

\subsubsection{Rangsatz}

\skript{2.27} Für jeden $K$-Vektorraumhomomorphismus $\varphi: V \to W$ gilt: $$\dim_K(V) = \rk_K(\varphi) + \defekt_K(\varphi)$$

\subsubsection{Verschiedenes}

Es seien $K$-Vektorräume $V$ und $W$, ein $K$-Vektorraumhomomorphismus $\varphi: V \to W$ und ein Tupel $s \in K^n$.

\skript{HA. 24 c)} $\varphi(\langle s_1,\ldots,s_n \rangle) = \langle \varphi(s_1), \ldots, \varphi(s_n) \rangle$

\skript{HA. 24 d),e)} Wenn $(\varphi(s_1),\ldots,\varphi(s_n))$ linear unabhängig ist, dann ist auch $(s_1,\ldots,s_n)$ linear Unabhängig. Ist $\varphi$ injektiv, so gilt auch die Umkehrung.

\section{Matrixkalkül}

\subsection{Wiederholung Diskrete Strukturen}

\subsubsection{Zeilen-/Spaltenoperatoren}

Eine Zeilenoperation ist eine Verkettung von Elementaren Zeilenoperatoren.
Eine Zeilenoperation $z$ hat eine eindeutige Matrix $Z = z(\mathrm{E})$, sodass $z(A) = ZA$. (E ist die Einheitsmatrix).	\\
Diese Matrix nennen wir Zeilenoperator. Der Zeilenoperator $Z$ einer Zeilenoperation $z = z_1 \circ \ldots \circ z_n$ mit Zeilenoperationen $z_1,\ldots,z_n$ ist das Produkt $Z_1 \cdot \ldots \cdot Z_n$ der Zeilenoperatoren zu $z_1,\ldots,z_n$.	\\
Für Spaltenoperationen $z$ sind Spaltenoperatoren $Z = z(\mathrm{E})$ analog, nur dass $z(A) = AZ$ (das Z steht auf der rechten Seite) und dass der Zeilenoperator zu $z_1 \circ \ldots \circ z_n$ $Z_n \cdot \ldots \cdot Z_1$ ist.

\subsection{Spalteninterpretation}

% Größe von B?
Für $m,n \in \mathbb{N}_0, A \in K^{m \times n}$:

\skript{3.1} $\varphi_A: K^{n \times 1} \to K^{m \times 1}, x \mapsto Ax$

\skript{3.4} $\varphi_A(x) = \sum_{j =1}^n  x_j A_{-,j}$

\skript{3.5 a} $\varphi_{BA}=\varphi_B \circ \varphi_A$

\skript{3.5 b} $\varphi_{\mathrm{E}_n} = \id_{K^{n \times 1}}$

\skript{3.5 c} $\varphi_{A}^{-1}=\varphi_{A^{-1}}$

\subsection{Koordinatenspalte}

\skript{3.6}$\kappa_s : V \to K^{n \times 1}, v \mapsto \kappa_s(v)$ \\
ist der zu $K^{n \times 1} \to V, a \mapsto \sum_{i \in [1,n]} a_i s_i$ inverse VR.-Hom.

\textit{Beispiel.} Koordinatenspalte von $a \in K^2$ bzgl. $s = ((1,0), (1,1))$ ist $$\kappa_s(a) = \begin{pmatrix} a_1 - a_2 \\ a_2 \end{pmatrix}$$ denn $a = (a_1, a_2) = (a_1 - a_2) (1,0) + a_2 (1,1)$. Dazu einfach das LGS lösen mit $$\sum_{i \in [1,n]} (\kappa_s(v))_i s_i = v$$ d.h. die Basis spaltenweise in die LGS-Matrix eintragen und lösen.

\subsection{Darstellungsmatrix}

\skript{3.8}$\mathrm{M}_{t,s}(\varphi) = (\kappa_t(\varphi(s_1)) \ldots \kappa_t(\varphi(s_n)))$ \\
für $\varphi : V \to W \in \mathrm{Hom}(V, W)$, Basen $s = (s_1, \ldots, s_n)$ von $V$ und $t = (t_1, \ldots, t_m)$ von $W$.

Es muss also für jede Spalte das $s_i$ in $\varphi$ eingesetzt werden und das Ergebnis wieder nach den Linearfaktoren von $t$ aufgelöst werden.

\skript{Tutoraufgabe 30.5.16 a}$\mathrm{M}_{e,e}(\varphi_A) = A$

\subsection{Basiswechselmatrix, Basiswechselformeln}

\skript{3.15}In einem Vektorraum $V$ mit Basen $s = (s_1,\ldots, s_n)$ und $s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ ist $\mathrm{M}_{s, s^\prime}(\id_V)$ die Basiswechselmatrix von $s$ nach $s^\prime$.

\skript{3.18a}Mit Basen $s = (s_1,\ldots, s_n)$ und $s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ von $V$ gilt für $v \in V$:\\ $\kappa_{s^\prime}(v) = (\mathrm{M}_{s,s^\prime}(\id_V))^{-1} \kappa_s(v)$.

\skript{3.18b}Mit $\varphi : V \to W \in \mathrm{Hom}(V,W)$, Basen $s = (s_1,\ldots, s_n)$ und $s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ von $V$ und Basen $t = (t_1,\ldots, t_n)$ und $t^\prime = (t^\prime_1, \ldots, t^\prime_n)$ von $W$ gilt: \\
$\mathrm{M}_{t^\prime, s^\prime}(\varphi) = (\mathrm{M}_{t,t^\prime}(\id_W))^{-1} \mathrm{M}_{t,s}(\varphi) \mathrm{M}_{s,s^\prime}(\id_V)$

\subsection{Kardinalitäten von versch. Mengen}

$$\abs{\Sol(A,0)} \text{ für ein } A \in \Field_q^{m \times n} \text{ mit } \rk_{\Field_q} A = r \text{ ist } q^{n-r}$$
Da Elemente aus $\Sol(A,0)$ Vektoren aus $\Field_q^n$ sind, müssen wir $n$ Parameter aus $\Field_q$ wählen, von denen $r$ nicht frei sind.

$$\abs{\{ U \leq \Field_q^n | \dim_{\Field_q} U = k \}} = \binom{n}{k}_q$$
Der Gauß'sche Binomialkoeffizient (``QBinomial'' in Mathematica). Ja, die Ergebnisse werden schnell groß.

$$\abs{\mathrm{Hom}(\Field_q^n,\Field_q^m)} = \abs{\Field_q^{m \times n}} = q^{m \cdot n}$$
Homomorphismen lassen sich als Matrizen darstellen und umgekehrt. Daher das ganze Matrixkalkül-Ding.

$$\abs{\GL_n(\Field_q)} = \prod_{i=0}^{n-1} q^n-q^i$$
$$\abs{\{A \in \Field_q^{m \times n} ~ | ~ \rk(A) = \min(m,n)\}} = \prod_{i=0}^{\min(m,n)-1} (q^{\max(m,n)}-q^i)$$
Oben müssen wir Zeilen oder Spalten linear unabhängig wählen, genau $\min(m,n)$ Stück mit Länge $\max(m,n)$. Die Erste ist dann bis auf die Null frei zu wählen, danach sollte jede folgende kein Vielfaches der Vorherigen sein.

\section{Lineare Kodierungstheorie}

\subsection{Blockcode}

\skript{4.1}Blockcode: Nicht-leere Teilmenge C von $K^{n \times 1}$ über $K$ mit Länge $n$. Elemente werden Codewörter genannt.

\skript{4.3}Hamming-Gewicht: $\mathrm{wt}(a):=|\{i \in [1,n]|a_i \neq 0\}|$ mit $a \in K^{n \times 1}$
\begin{itemize}
	\item $\mathrm{wt}(a+b) \leq \mathrm{wt}(a)+ \mathrm{wt}(b)$
\end{itemize}

\skript{4.6}Hamming-Abstand: $d(a,b):=wt(b-a)$
\begin{itemize}
	\item $\mathrm{d}(a,b)=|\{i \in [1,n]|a_i \neq b_i\}|$
	\item $\mathrm{d}(a,c) \leq \mathrm{d}(a,b)+\mathrm{d}(b,c)$
\end{itemize}

\skript{4.10}(Offene) Kugel: $\mathrm{B}_r(a):=\{x \in K^{n \times 1}|\mathrm{d}(a,x) < r\}$ mit $r \in \mathbb{R}_{\geq 0}$

\subsection{Minimalabstand eines Blockcodes}

\skript{4.11}Minimalabstand: $\mathrm{d}(C):=\min\{\mathrm{d}(c,c^\prime)|c,c^\prime \in C$ mit $c \neq c^\prime \}$, falls $|C| \ge 1$
\begin{itemize}
	\item $\mathrm{B}_{\mathrm{d}(C)}(c) \cap C = \{c\}$
\end{itemize}

\skript{4.15}Nächster Nachbar von $x$ ist $c$ mit $\mathrm{d}(c,x)=\min\{d(c^\prime,x|c^\prime \in C)\}$
\begin{itemize}
	\item $\forall x \in \mathrm{B}_{\frac{\mathrm{d}(C)}{2}}(c)$ ist $c$ der eindeutige nächste Nachbar von $x \in C$
\end{itemize}

\subsection{Lineare Codes}

\skript{4.19}$C$ ist ein linearer $[n,k,d]$-Code, wenn die Länge $n$, $\dim_K C=k$ und $\mathrm{d}(C)=d$ ist.

\skript{4.21}$\mathrm{d}(C) = \min \{\mathrm{wt}(c)|c \in C\setminus\{0\}\}$, falls $C \neq \{0\}$

\subsection{Erzeugermatrizen}

\skript{4.22}Eine Erzeugermatrix ist $A \in K^{n \times k}$ mit $C=\Col(A)$, wenn C ein linearer $[n,k]$-Code über $K$ ist.

\skript{4.24}$\rk_k A=k$, falls $A$ Erzeugermatrix für linearen  $[n,k]$-Code $C$.

\skript{4.25}Anwendung:
\begin{itemize}
	\item Kodieren von $w \in K^{k \times 1}$ zu einem Codewort mit Erzeugermatrix $A$: \\
	Berechne $Aw$.
	\item Dekodieren eines Codewortes $c \in C$ zu einem Informationswort bzgl. $A$.: \\
	Berechne $w \in K^{k \times 1}$ mit $Aw=c$.
\end{itemize}

\subsection{Kontrollmatrizen}

\skript{4.26}Kontrollmatrix ist $B \in K^{(n-k) \times n}$ mit $C=Sol(B,0)$, wenn $k \in [0,n], C$ ein linearer $[n,k]$-Code über $K$ ist.
\begin{itemize}
	\item Berechnung der Kontrollmatrix $C$ mit Erzeugermatrix $A: Sol(A^{tr},0)=\langle u,..,v\rangle=Col(B^{tr})$
	\item $Sol(B,0)=Col(A)=C$
\end{itemize}

\skript{4.28}$rk_K B=n-k$

\skript{4.29}Codewort-Test: Berechne $Bx$ und falls $Bx=0$ ist, dann ist $x$ ein Codewort von $C$.

\skript{4.30}Seien $k \in [1,n],$ ein linearer $[n,k]$-Code $C$ über $K$ und die Kontrollmatrix $B$ gegeben. Dann ist \\
$\mathrm{d}(C) = \min \{r \in \mathbb{N}_0 \ |\ $ es gibt $j_1,...,j_r \in [1,n]$ mit $j_1 <...<j_r$ so, dass $(B_{-,j_1},...,B_{-,j_r})$ linear abhängig ist. \\
=$ \max \{r \in \mathbb{N}_0 \ |\ $ für alle $j_1,...,j_r \in [1,n]$ mit $j_1 <...<j_r$ ist $(B_{-,j_1},...,B_{-,j_r})$ linear unabhängig.

\subsection{Syndromdekodierung}

\skript{4.31}Syndrom: Für $x \in K^{n \times 1}$ heißt $Bx \in K^{(n-k) \times 1}$ das Syndrom von $x$ bzgl. $B$, mit $k \in [1,n]$ und einem linearen $[n,k]$-Code $C$ über $K$.

\skript{4.33}Sei $y$ das Syndrom von $x$. Für $x` \in K^{n \times 1}$ gilt: $x`$ hat das Syndrom $y \Leftrightarrow x`-x$ ist ein Codewort von $C$.

\skript{4.34}Ein Anführer für $y$ bzgl. $B$ ist ein $e \in K^{n \times 1}$ mit Syndrom $y$ und $\mathrm{wt}(e)=\min\{\mathrm{wt}(x)\ |\ x \in K^{n \times 1}$ so, dass $x$ das Syndrom $y$ hat$\}$.

\skript{4.36}Codewort $c$ ist ein nächster Nachbar von $x$ in $C \Leftrightarrow x-c$ ist Anführer für $y$.

\skript{4.37}$\forall x \in B_{\frac{d(C)}{2}}(c)$ ist $x-c$ der eindeutige Anführer für das Syndrom von $x$.

\skript{4.38}Syndromdekodierung: \\

Initialisierung:
\begin{itemize}
	\item Wähle $k \in [0,n]$ und einen libnearen $[n,k]$-Code $C$ über $K$, mit weniger als $\frac{d(C)}{2}$ Fehlern.
	\item Bestimme die Kontrollmatrix $B$ für $C$
	\item Bestimme $\forall y \in K^{(n-k) \times 1}$ einen Anführer $e_y$ und lege ein Wörterbuch an
\end{itemize}
Dekodierung von $x \in K^{n \times 1}$:
\begin{itemize}
	\item Berechne das Syndrom $Bx$ von $x$ bzgl. $B$
	\item Suche Fehlwort $e_{Bx}$ aus Wörterbuch
	\item Dekodiere $x$ zu $x-e_{Bx}$
\end{itemize}

\section{Determinante}

Die Determinante gibt Auskunft über quadratische Matrizen. Wenn sie invertierbar ist, dann ist auch die Matrix invertierbar. In Körpern heißt das einfach ungleich Null, in Ringen muss Invertierbarkeit geprüft werden. Im Folgenden ist $R$ ein beliebiger kommutativer und unitärer Ring. \\

Die Leibniz-Formel für kleine Matrizen:

\skript{5.2a} Für $A \in R^{0 \times 0}$: $\det \begin{pmatrix} & \end{pmatrix} = 1$

\skript{5.2b} Für $A \in R^{1 \times 1}$: $\det A = A_{1,1}$

\skript{5.3c} Für $A \in R^{2 \times 2}$: $\det A = A_{1,1} A_{2,2} - A_{1,2} A_{2,1}$ \\

Die Leibniz-Formel für $R^{3 \times 3}$-Matrizen ist nicht schön und wird nicht schöner in höhreren Dimensionen. Dazu der Kästchensatz: \\

\skript{5.15}\textit{Kästchensatz.} Für Matrizen $A \in R^{m \times m}$, $B \in R^{m \times n}$, $C \in R^{n \times n}$ gilt: \\
$\det \begin{pmatrix} A & B \\ 0 & C \end{pmatrix} = (\det A) (\det C)$

Dieser ist der beste Ansatz, um Determinanten per Hand auszurechen: Man vereinfacht die Matrix (siehe Abschnitt zur Veränderung der Matrix) und dann wendet man den Kästchensatz an.

\subsection{Produktsatz}

\skript{5.8a} $\det (B A) = (\det B) (\det A)$

\skript{5.8b} $\det \textrm{E}_n = 1$

\skript{5.8c} $(\det A)^{-1} = \det (A^{-1})$

\subsection{Veränderung der Matrix und ihrer Determinante}

Nach diesem Satz gelten die folgenden Sätze für Zeilen analog wie für Spalten:

\skript{5.4} $\det A^\mathrm{tr} = \det A$ \\

\skript{5.5/1}\textit{Multilinearität.} \\
$\det \begin{pmatrix} x_1 & \ldots & x_{k-1} & y+z & x_{k+1} & \ldots  & x_n \end{pmatrix} = \\
 \det \begin{pmatrix} x_1 & \ldots & x_{k-1} & y & x_{k+1} & \ldots  & x_n \end{pmatrix} +
 \det \begin{pmatrix} x_1 & \ldots & x_{k-1} & z & x_{k+1} & \ldots  & x_n \end{pmatrix}$
Außerdem: \\
$\det \begin{pmatrix} x_1 & \ldots & x_{k-1} & ay & x_{k+1} & \ldots  & x_n \end{pmatrix} =
a \det \begin{pmatrix} x_1 & \ldots & x_{k-1} & y & x_{k+1} & \ldots  & x_n \end{pmatrix}$

\skript{5.5/2} Wenn alle Spalten paarweise gleich sind, dann gilt $\det A = 0$\\

Um die Determinante deutlich einfacher berechnen zu können, ist es möglich, die Matrix mit unseren elementaren Zeilen- und Spaltenoperationen zu vereinfachen. Dazu gibt es Sätze für jeden Operator, die im Skript etwas umständlicher formuliert sind (und auch für Spalten definiert), aber trotzdem hier genau so gelten:

\skript{5.6a} Mit $k \neq l$: $\det \textrm{sw}_{k,l}(A) = - \det A$

\skript{5.6b} Mit $a \in R$ und $k \neq l$: $\det \textrm{add}_{k,l,a}(A) = \det A$

\skript{5.6c} Mit $a \in R$: $\det \textrm{mul}_{k,a}(A) = a \det A$

\subsection{Laplace-Entwicklung}

Determinanten größerer Matrizen werden in der Praxis mit der Laplace-Entwicklung rekursiv berechnet. Diese sollte man per Hand aber wegen der Fehleranfälligkeit besser zu Gunsten der Vereinfachung und Verwendung obiger Sätze nicht verwenden.
Zuerst braucht man für die Laplace-Entwicklung die Definition des Minor:

\skript{5.11} Der Minor an der Stelle $(k,l)$ mit $k,l \in [1,n]$ von $A \in R^{n \times n}$ ist: \\
$$\textrm{Minor}_{k,l}(A) := \det \begin{pmatrix}
A_{1,1} & \ldots & A_{1,l-1} & A_{1,l+1} & \ldots & A_{1,n} \\
\vdots & & \vdots & \vdots & & \vdots \\
A_{k-1,1} & \ldots & A_{k-1,l-1} & A_{k-1,l+1} & \ldots & A_{k-1,n} \\
A_{k+1,1} & \ldots & A_{k+1,l-1} & A_{k+1,l+1} & \ldots & A_{k+1,n} \\
\vdots & & \vdots & \vdots & & \vdots \\
A_{n,1} & \ldots & A_{n,l-1} & A_{n,l+1} & \ldots & A_{n,n} \\
\end{pmatrix}$$

Es ist etwas schwer zu erkennen, aber der Minor an irgendeiner Stelle bedeutet ledglich: Berechne die Determinante der Matrix ohne Zeile $k$ und ohne Spalte $l$.

Die Laplace-Entwicklung ist dann möglich entweder nach Spalte $l$ oder nach Zeile $k$:

\skript{5.13a} Für $l \in [1,n]$ gilt: \\
$$\det A = \sum_{k=1}^n (-1)^{k+l} A_{k,l} \textrm{Minor}_{k,l}(A)$$

\skript{5.13b} Für $k \in [1,n]$ gilt: \\
$$\det A = \sum_{l=1}^n (-1)^{k+l} A_{k,l} \textrm{Minor}_{k,l}(A)$$

Da gilt es besonders hart drauf zu starren, bis sich einem das System ergibt. Oder einach weiterlesen, denn es folgt eine kurze Erklärung:

\begin{enumerate}
\setcounter{enumi}{-1}
\item \textit{(Pro-Tipp)} Vorher die Matrix mit elementaren Zeilenoperationen vereinfachen. Das verändert aber die Determinante etwas, siehe Korollar (5.6).
\item Eine Zeile oder Spalte aussuchen, nach der man entwickeln möchte. Mit Vorraussicht auf die nächsten Rechnungen sollte die Zeile/Spalte mit den meisten Nullen gewählt werden, das spart gleich Arbeit.
\item Jedes Element der gewählten Zeile/Spalte wird dann mit einem Minor multipliziert. Der Minor ist einfach die Determinante, wo die Entwicklungsspalte oder -zeile und die Zeile/Spalte des aktuellen Elementes weggestrichen wurden. Hier profitiert man von den Nullen.
\item Wenn die Minoren mit großen Matrizen zu berechnen sind, einfach rekursiv weiter Laplace-Entwickeln!
\item Die Ergebnisse der Minoren multipliziert mit dem Wert alternierend addieren. Ob man mit $(-1)$ oder $(+1)$ anfängt, hängt davon ab, welchen Index die Zeile/Spalte hat, nach der man entwickelt: Gerader Index $\implies (-1)$
\end{enumerate}

\subsection{Determinante eines Vektorraumendomorphismus}

Man beachte: Vektorraum\textbf{\underline{endo}}morphismus. \\

Die Determinante ist unabhängig von den Basen der Darstellungsmatrix:

\skript{5.9}Für  $\varphi : V \to V$, Basen $s = (s_1, \ldots, s_n), s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ von $V$ gilt: \\
$\det \textrm{M}_{s,s}(\varphi) = \det \textrm{M}_{s^\prime, s^\prime}(\varphi)$ \\

Damit können wir auch problemlos $\det \textrm{M}_{s,s}(\varphi)$ mit $\det \varphi$ abkürzen.

\section{Eigenwerte}

\skript{6.1} Ein Eigenvektor $v$ eines Endomorphismus' $\varphi$ ist ein nicht-null Vektor, der von dem Endomorphismus bis auf einen skalaren Faktor, den Eigenwert, nicht verändert wird. Ein Eigenraum ist der Vektorraum, der alle Eigenvektoren zu einem Eigenwert (und den Nullvektor) enthält.
$$\Eig_a(\varphi) = \{ v \in V \vert \varphi(v) = a v \}$$

Die Eigenwerte/-vektoren/-räume einer Matrix $A$ sind die Eigenwerte/-vektoren/-räume von $\varphi_A$.

\skript{6.6} Für verschiedene Eigenwerte sind die zugehörigen Eigenräume unabhängig.

\subsection{Characteristische Polynome}

\skript{6.8} Das charakteristische Polynom einer Matrix $A$ ist $\chi_A = \det(XE_n-A)$.

\skript{6.12} Das charakteristische Polynom eines Vektorraumendomorphismus' $\varphi$ ist $\chi_\varphi = \chi_{M_{s,s}(\varphi)}$ für eine beliebige Basis $s$.

\skript{6.13} Es sind Äquivalent:
\begin{enumerate}
	\item $a$ ist ein Eigenwert von $A$
	\item $\rk(A-aE_n)<n$
	\item $det(A-aE_n)=0$
	\item $a$ ist eine Nullstelle des Charakteristischen Polynoms von $A$
\end{enumerate}

\subsubsection{Begleitmatrix}

\skript{6.20} Für ein normiertes Polynom $f = X^n + \sum_{i=0}^{n-1} a_iX^i \in K[X]\setminus\{0\}$ ist die Begleitmatrix
$$C(f) = \left(\begin{array}{cccc}
		& 0 & & -a_0	\\
		& & & -a_1	\\
		& E_{n-1} & & \vdots	\\
		& & & -a_n
\end{array}\right)$$

\skript{6.22} Das charakteristische Polynom einer Begleitmatrix ist
$$\chi_{C(f)} = f$$

\subsection{Vielfachheiten}

\skript{6.16} Die geometrische Vielfachheit eines Eigenwertes $a$ von einem Endomorphismus $\varphi$ ist
$$\geo_a(\varphi) = \dim(\Eig_a(\varphi))$$
	Die algebraische Vielfachheit eines Eigenwertes $a$ von einem Endomorphismus $\varphi$ ist
$$\mul_a(\varphi) = \mul_a(\chi_\varphi)$$
	Für eine Matrix $A$ sind Vielfachheiten analog über $\varphi_A$ gegeben.

\skript{6.19} Es gilt stets $\geo_a(\varphi) \le \mul_a(\varphi)$

\subsection{Diagonalisierbarkeit}

\skript{6.41} Für einen Endomorphismus $\varphi: V \to V$ und eine Basis $s$ ist $s$ eine Eigenbasis von $V$ bezüglich $\varphi$ wenn alle Einträge von $s$ Eigenvektoren von $\varphi$ sind.	\\
Für eine Matrix $A \in K^{n \times n}$ und eine Basis $s$ von $K^{n \times 1}$ heißt $s$ Eigenbasis von $K^{n \times 1}$ bezüglich $A$, wenn $s$ eine Eigenbasis bezüglich $\varphi_A$ ist.

\skript{6.43} Für einen Endomorphismus $\varphi: V \to V$ ist eine Basis $s$ von $V$ genau dann eine Eigenbasis bezüglich $\varphi$ wenn $M_{s,s}(\varphi)$ eine Diagonalmatrix ist.

\skript{6.46} Ein Endomorphismus $\varphi$ heißt diagonalisierbar, falls es eine Eigenbasis bezüglich $\varphi$ gibt.	\\
Eine Matrix $A$ heißt diagonalisierbar, wenn $\varphi_A$ diagonalisierbar ist.

\skript{6.50} Für eine Matrix $A \in K^{n \times n}$ sind äquivalent:
\begin{enumerate}
	\item $A$ ist diagonalisierbar.
	\item Es gibt eine Eigenbasis bezüglich $A$.
	\item $A$ ist ähnlich zu einer Diagonalmatrix.
\end{enumerate}

\skript{6.52} Für einen Endomorphismus $\varphi: V \to V$ sind äquivalent:
\begin{enumerate}
	\item $\varphi$ ist diagonalisierbar.
	\item $V = \dot\sum_{a \in K\text{, a ist Eigenwert von $\varphi$}} \Eig_a(\varphi)$.
	\item $\dim V = \sum_{a \in K} \geo_a(\varphi)$.
	\item Das charakteristische Polynom $\chi_\varphi$ zerfällt in Linearfaktoren und für jeden Eigenwert $a$ von $\varphi$ gilt $\geo_a(\varphi) = \mul_a(\varphi)$.
\end{enumerate}

\skript{6.53} Wenn ein Endomorphismus $\varphi: V \to V$ $\dim V$ verschiedene Eigenwerte hat, ist er diagonalisierbar.

\skript{6.55} Für $f \in K[X]$ und $A \in K^{n \times n}$ mit einem $P \in \GL_n(K)$ sodass
$$P^{-1} A P = \left(\begin{array}{ccc}
		a_0 & & 0	\\
		& \ddots &	\\
		0 & & a_n
	\end{array}\right)$$
gilt:
$$f(A) = P^{-1} \left(\begin{array}{ccc}
		f(a_0) & & 0	\\
		& \ddots &	\\
		0 & & f(a_n)
	\end{array}\right) P $$

\subsection{Invariante Untervektorräume}

\skript{6.76} Für einen $K$-Vektorraumendomorphismus $\varphi: V \to V$ heißt ein Untervektorraum $U$ von $V$ invariant unter $\varphi$ wenn
$$\varphi(U) \subseteq U$$
gilt. Für eine Matrix $A \in K^{n \times n}$ heißt ein Untervektorraum $U$ von $K^{n \times 1}$ invariant unter $A$, wenn er invariant unter $\varphi_A$ ist.

\skript{6.78} Für einen $K$-Vektorraumendomorphismus $\varphi: V \to V$ und einen Untervektorraum $U$ von $V$ mit einer Basis $s = (s_1,\ldots, s_n)$, sodass $s^\prime = (s_1,\ldots,s_m)$ ($m \leq n$) eine Basis von $U$ ist, ist $U$ invariant unter $\varphi$ genau dann, wenn die Darstellungsmatrix von $\varphi$ zu $s$ die Form
$$M_{s,s}(\varphi) = \left(\begin{array}{cc}
		A & B	\\
		0 & C
	\end{array}\right)$$
mit $A \in K^{m \times m}$ hat. In diesem Fall ist $M_{s^\prime,s^\prime}(\varphi\vert_U^U) = A$.

\skript{6.79} Für einen $K$-Vektorraumendomorphismus $\varphi: V \to V$ und einen $\varphi$-invarianten Untervektorraum $U$ von $V$ gilt
$$\chi_{\varphi\vert_U^U} \vert \chi_\varphi$$

\subsubsection{Satz von Cayley/Hamilton}
\skript{6.80}\begin{enumerate}
	\item Für einen $K$-Vektorraumendomorphismus $\varphi: V \to V$ ist
		$$\chi_\varphi(\varphi) = 0$$
	\item Für eine Matrix $A \in K^{n \times n}$ ist
		$$\chi_A(A) = 0$$
\end{enumerate}

\end{document}
