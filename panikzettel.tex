\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hanging}
\usepackage{calc}
\usepackage[ngerman]{babel}
\title{\textbf{Lineare Algebra Panikzettel}}
\author{Philipp Schröer, Tobias Polock, Caspar Zecha}
\date{\today}

% Formatting
\setlength\parindent{0pt}
\relpenalty=9999
\binoppenalty=9999

% Code blocks
\lstnewenvironment{mat}
{\lstset{language=Mathematica,
	mathescape,
    columns=flexible,
    backgroundcolor=\color{lightgray!10},
%    numberstyle=\tiny\color{gray},
%    numbers=left,
    basicstyle=\small\ttfamily}
    }
{}

% Definitions
\newlength{\hangwidth}
\newcommand{\skript}[1]{\settowidth{\hangwidth}{\textbf{(#1)} }\hangpara{\hangwidth}{1}\textbf{(#1)} }

\newcommand{\id}{\mathrm{id}}
\newcommand{\Sol}{\mathrm{Sol}}
\newcommand{\Col}{\mathrm{Sol}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\Field}{\mathbb{F}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}


\begin{document}
\maketitle

\section{Vektorräume}

$K$-Vektorraum $V$ hat die Operationen Addition  $V \times V \to V, (v,w) \mapsto v+w$, sowie die Skalarmultiplikation $K \times V \to V, (a,v) \mapsto a \cdot v  $, für den folgende Axiome mit $v, w, x \in V, a,b \in K$ gelten müssen:

\begin{enumerate}
\item Assoziativität d. Addition: $(v+w)+x = v+(w+x)$
\item Nullvektor: $\exists 0 \in V: v+0=0+v=v$
\item Negative: $(-v)+v=v+(-v)=0$
\item Kommutative Additon: $v+w=w+v$
\item Assoziative Skalarmultiplikation: $a(bv)=(ab)v$
\item Einselement: $1\cdot v=v$
\item Distributivität: $(a+b)v=(av)+(bv)$, sowie $a(v+w)=(av)+(aw)$
\end{enumerate}

\section{Matrixkalkül}

\subsection{Wiederholung Diskrete Strukturen}

\subsubsection{Zeilen-/Spaltenoperatoren}

Eine Zeilenoperation ist eine Verkettung von Elementaren Zeilenoperatoren.
Eine Zeilenoperation $z$ hat eine eindeutige Matrix $Z = z(\mathrm{E})$, sodass $z(A) = ZA$. (E ist die Einheitsmatrix).	\\
Diese Matrix nennen wir Zeilenoperator. Der Zeilenoperator $Z$ einer Zeilenoperation $z = z_1 \circ \ldots \circ z_n$ mit Zeilenoperationen $z_1,\ldots,z_n$ ist das Produkt $Z_1 \cdot \ldots \cdot Z_n$ der Zeilenoperatoren zu $z_1,\ldots,z_n$.	\\
Für Spaltenoperationen $z$ sind Spaltenoperatoren $Z = z(\mathrm{E})$ analog, nur dass $z(A) = AZ$ (das Z steht auf der rechten Seite) und dass der Zeilenoperator zu $z_1 \circ \ldots \circ z_n$ $Z_n \cdot \ldots \cdot Z_1$ ist.

\subsection{Spalteninterpretation}

% Größe von B?
Für $m,n \in \mathbb{N}_0, A \in K^{m \times n}$:

\skript{3.1} $\varphi_A: K^{n \times 1} \to K^{m \times 1}, x \mapsto Ax$

\skript{3.4} $\varphi_A(x) = \sum_{j =1}^n  x_j A_{-,j}$

\skript{3.5 a} $\varphi_{BA}=\varphi_B \circ \varphi_A$

\skript{3.5 b} $\varphi_{\mathrm{E}_n} = \id_{K^{n \times 1}}$

\skript{3.5 c} $\varphi_{A}^{-1}=\varphi_{A^{-1}}$

\subsection{Koordinatenspalte}

\skript{3.6}$\kappa_s : V \to K^{n \times 1}, v \mapsto \kappa_s(v)$ \\
ist der zu $K^{n \times 1} \to V, a \mapsto \sum_{i \in [1,n]} a_i s_i$ inverse VR.-Hom.

\textit{Beispiel.} Koordinatenspalte von $a \in K^2$ bzgl. $s = ((1,0), (1,1))$ ist $$\kappa_s(a) = \begin{pmatrix} a_1 - a_2 \\ a_2 \end{pmatrix}$$ denn $a = (a_1, a_2) = (a_1 - a_2) (1,0) + a_2 (1,1)$. Dazu einfach das LGS lösen mit $$\sum_{i \in [1,n]} (\kappa_s(v))_i s_i = v$$ d.h. die Basis spaltenweise in die LGS-Matrix eintragen und lösen.

\begin{mat}
> LinearSolve[{{1, 0}, {1, 1}} // Transpose, {a1, a2}]
{a1-a2,a2}
\end{mat}

\subsection{Darstellungsmatrix}

\skript{3.8}$\mathrm{M}_{t,s}(\varphi) = (\kappa_t(\varphi(s_1)) \ldots \kappa_t(\varphi(s_n)))$ \\
für $\varphi : V \to W \in \mathrm{Hom}(V, W)$, Basen $s = (s_1, \ldots, s_n)$ von $V$ und $t = (t_1, \ldots, t_m)$ von $W$.

Es muss also für jede Spalte das $s_i$ in $\varphi$ eingesetzt werden und das Ergebnis wieder nach den Linearfaktoren von $t$ aufgelöst werden.

\textit{Beispiel.}
\begin{mat}
> phi[{x1_, x2_}] := {x1+x2, x1-x2, x2}
> s = {{1, 0}, {0, 1}}
> t = {{1,0,0}, {0, 1, 0}, {0, 0, 1}}
> {
	LinearSolve[t // Transpose, phi[s[[1]]]],
	LinearSolve[t // Transpose, phi[s[[2]]]]
  } // Transpose
{
  {1, 1},
  {1, -1},
  {0, 1}
}
\end{mat}

\skript{Tutoraufgabe 30.5.16 a}$\mathrm{M}_{e,e}(\varphi_A) = A$

\subsection{Basiswechselmatrix, Basiswechselformeln}

\skript{3.15}In einem Vektorraum $V$ mit Basen $s = (s_1,\ldots, s_n)$ und $s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ ist $\mathrm{M}_{s, s^\prime}(\id_V)$ die Basiswechselmatrix.

\skript{3.18a}Mit Basen $s = (s_1,\ldots, s_n)$ und $s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ von $V$ gilt für $v \in V$:\\ $\kappa_{s^\prime}(v) = (\mathrm{M}_{s,s^\prime}(\id_V))^{-1} \kappa_s(v)$.

\skript{3.18b}Mit $\varphi : V \to W \in \mathrm{Hom}(V,W)$, Basen $s = (s_1,\ldots, s_n)$ und $s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ von $V$ und Basen $t = (t_1,\ldots, t_n)$ und $t^\prime = (t^\prime_1, \ldots, t^\prime_n)$ von $W$ gilt: \\
$\mathrm{M}_{t^\prime, s^\prime}(\varphi) = (\mathrm{M}_{t,t^\prime}(\id_W))^{-1} \mathrm{M}_{t,s}(\varphi) \mathrm{M}_{s,s^\prime}(\id_V)$

\subsection{Kardinalitäten von versch. Mengen}

$$\abs{\Sol(A,0)} \text{ für ein } A \in \Field_q^{m \times n} \text{ mit } \rk_{\Field_q} A = r \text{ ist } q^{n-r}$$
Da Elemente aus $\Sol(A,0)$ Vektoren aus $\Field_q^n$ sind, müssen wir $n$ Parameter aus $\Field_q$ wählen, von denen $r$ nicht frei sind.

$$\abs{\{ U \leq \Field_q^n | \dim_{\Field_q} U = k \}} = \binom{n}{k}_q$$
Der Gauß'sche Binomialkoeffizient (``QBinomial'' in Mathematica). Ja, die Ergebnisse werden schnell groß.

$$\abs{\mathrm{Hom}(\Field_q^n,\Field_q^m)} = \abs{\Field_q^{m \times n}} = q^{m \cdot n}$$
Homomorphismen lassen sich als Matrizen darstellen und umgekehrt. Daher das ganze Matrixkalkül-Ding.

$$\abs{\GL_n(\Field_q)} = \prod_{i=0}^{n-1} q^n-q^i$$
$$\abs{\{A \in \Field_q^{m \times n} ~ | ~ \rk(A) = \min(m,n)\}} = \prod_{i=0}^{\min(m,n)-1} (q^{\max(m,n)}-q^i)$$
Oben müssen wir Zeilen oder Spalten linear unabhängig wählen, genau $\min(m,n)$ Stück mit Länge $\max(m,n)$. Die Erste ist dann bis auf die Null frei zu wählen, danach sollte jede folgende kein Vielfaches der Vorherigen sein.

\section{Lineare Kodierungstheorie}

\subsection{Blockcode}

\skript{4.1}Blockcode: Nicht-leere Teilmenge C von $K^{n \times 1}$ über $K$ mit Länge $n$. Elemente werden Codewörter genannt.

\skript{4.3}Hamming-Gewicht: $\mathrm{wt}(a):=|\{i \in [1,n]|a_i \neq 0\}|$ mit $a \in K^{n \times 1}$
\begin{itemize}
	\item $\mathrm{wt}(a+b) \leq \mathrm{wt}(a)+ \mathrm{wt}(b)$
\end{itemize}

\skript{4.6}Hamming-Abstand: $d(a,b):=wt(b-a)$
\begin{itemize}
	\item $\mathrm{d}(a,b)=|\{i \in [1,n]|a_i \neq b_i\}|$
	\item $\mathrm{d}(a,c) \leq \mathrm{d}(a,b)+\mathrm{d}(b,c)$
\end{itemize}

\skript{4.10}Kugel: $\mathrm{B}_r(a):=\{x \in K^{n \times 1}|\mathrm{d}(a,x) \leq r\}$ mit $r \in \mathbb{R}_{\geq 0}$

\subsection{Minimalabstand eines Blockcodes}

\skript{4.11}Minimalabstand: $\mathrm{d}(C):=\min\{\mathrm{d}(c,c^\prime)|c,c^\prime \in C$ mit $c \neq c^\prime \}$, falls $|C| \ge 1$
\begin{itemize}
	\item $\mathrm{B}_{\mathrm{d}(C)}(c) \cap C = \{c\}$
\end{itemize}

\skript{4.15}Nächster Nachbar von $x$ ist $c$ mit $\mathrm{d}(c,x)=\min\{d(c^\prime,x|c^\prime \in C)\}$
\begin{itemize}
	\item $\forall x \in \mathrm{B}_{\frac{\mathrm{d}(C)}{2}}$ ist $c$ der eindeutige nächste Nachbar von $x \in C$
\end{itemize}

\subsection{Lineare Codes}

\skript{4.19}$C$ ist ein linearer $[n,k,d]$-Code, wenn die Länge $n$, $\dim_K C=k$ und $\mathrm{d}(C)=d$ ist.

\skript{4.21}$\mathrm{d}(C) = \min \{\mathrm{wt}(c)|c \in C\setminus\{0\}\}$, falls $C \neq \{0\}$

\subsection{Erzeugermatrizen}

\skript{4.22}Eine Erzeugermatrix ist $A \in K^{n \times k}$ mit $C=\Col(A)$, wenn C ein linearer $[n,k]$-Code über $K$ ist.

\skript{4.24}$\rk_k A=k$, falls $A$ Erzeugermatrix für linearen  $[n,k]$-Code $C$.

\skript{4.25}Anwendung:
\begin{itemize}
	\item Kodieren von $w \in K^{k \times 1}$ zu einem Codewort mit Erzeugermatrix $A$: \\
	Berechne $Aw$.
	\item Dekodieren eines Codewortes $c \in C$ zu einem Informationswort bzgl. $A$.: \\
	Berechne $w \in K^{k \times 1}$ mit $Aw=c$.
\end{itemize}

\subsection{Kontrollmatrizen}

\skript{4.26}Kontrollmatrix ist $B \in K^{(n-k) \times n}$ mit $C=Sol(B,0)$, wenn $k \in [0,n], C$ ein linearer $[n,k]$-Code über $K$ ist.
\begin{itemize}
	\item Berechnung der Kontrollmatrix $C$ mit Erzeugermatrix $A: Sol(A^{tr},0)=\langle u,..,v\rangle=Col(B^{tr})$
	\item $Sol(B,0)=Col(A)=C$
\end{itemize}

\skript{4.28}$rk_K B=n-k$

\skript{4.29}Codewort-Test: Berechne $Bx$ und falls $Bx=0$ ist, dann ist $x$ ein Codewort von $C$.

\skript{4.30}Seien $k \in [1,n],$ ein linearer $[n,k]$-Code $C$ über $K$ und die Kontrollmatrix $B$ gegeben.

\subsection{Syndromdekodierung}

\skript{4.31}Syndrom: Für $x \in K^{n \times 1}$ heißt $Bx \in K^{(n-k) \times 1}$ das Syndrom von $x$ bzgl. $B$, mit $k \in [1,n]$ und einem linearen $[n,k]$-Code $C$ über $K$.

\skript{4.33}Sei $y$ das Syndrom von $x$. Für $x` \in K^{n \times 1}$ gilt: $x`$ hat das Syndrom $y \Leftrightarrow x`-x$ ist ein Codewort von $C$.

\skript{4.34}Ein Anführer für $y$ bzgl. $B$ ist ein $e \in K^{n \times 1}$ mit Syndrom $y$ und $\mathrm{wt}(e)=\min\{\mathrm{wt}(x)\ |\ x \in K^{n \times 1}$ so, dass $x$ das Syndrom $y$ hat$\}$.

\skript{4.36}Codewort $c$ ist ein nächster Nachbar von $x$ in $C \Leftrightarrow x-c$ ist Anführer für $y$.

\skript{4.37}$\forall x \in B_{\frac{d(C)}{2}}(c)$ ist $x-c$ der eindeutige Anführer für das Syndrom von $x$.

\skript{4.38}Syndromdekodierung: \\

Initialisierung:
\begin{itemize}
	\item Wähle $k \in [0,n]$ und einen libnearen $[n,k]$-Code $C$ über $K$, mit weniger als $\frac{d(C)}{2}$ Fehlern.
	\item Bestimme die Kontrollmatrix $B$ für $C$
	\item Bestimme $\forall y \in K^{(n-k) \times 1}$ einen Anführer $e_y$ und lege ein Wörterbuch an
\end{itemize}
Dekodierung von $x \in K^{n \times 1}$:
\begin{itemize}
	\item Berechne das Syndrom $Bx$ von $x$ bzgl. $B$
	\item Suche Fehlwort $e_{Bx}$ aus Wörterbuch
	\item Dekodiere $x$ zu $x-e_{Bx}$
\end{itemize}

\section{Determinante}

Die Determinante gibt Auskunft über quadratische Matrizen. Wenn sie invertierbar ist, dann ist auch die Matrix invertierbar. In Körpern heißt das einfach ungleich Null, in Ringen muss Invertierbarkeit geprüft werden. Im Folgenden ist $R$ ein beliebiger kommutativer und unitärer Ring. \\

Die Leibniz-Formel für kleine Matrizen:

\skript{5.2a} Für $A \in R^{0 \times 0}$: $\det \begin{pmatrix} & \end{pmatrix} = 1$

\skript{5.2b} Für $A \in R^{1 \times 1}$: $\det A = A_{1,1}$

\skript{5.3c} Für $A \in R^{2 \times 2}$: $\det A = A_{1,1} A_{2,2} - A_{1,2} A_{2,1}$ \\

Die Leibniz-Formel für $R^{3 \times 3}$-Matrizen ist nicht schön und wird nicht schöner in höhreren Dimensionen. Dazu der Kästchensatz: \\

\skript{5.15}\textit{Kästchensatz.} Für Matrizen $A \in R^{m \times m}$, $B \in R^{m \times n}$, $C \in R^{n \times n}$ gilt: \\
$\det \begin{pmatrix} A & B \\ 0 & C \end{pmatrix} = (\det A) (\det C)$

Dieser ist der beste Ansatz, um Determinanten per Hand auszurechen: Man vereinfacht die Matrix (siehe Abschnitt zur Veränderung der Matrix) und dann wendet man den Kästchensatz an.

\subsection{Produktsatz}

\skript{5.8a} $\det (B A) = (\det B) (\det A)$

\skript{5.8b} $\det \textrm{E}_n = 1$

\skript{5.8c} $(\det A)^{-1} = \det (A^{-1})$

\subsection{Veränderung der Matrix und ihrer Determinante}

Nach diesem Satz gelten die folgenden Sätze für Zeilen analog wie für Spalten:

\skript{5.4} $\det A^\mathrm{tr} = \det A$ \\

\skript{5.5/1}\textit{Multilinearität.} \\
$\det \begin{pmatrix} x_1 & \ldots & x_{k-1} & y+z & x_{k+1} & \ldots  & x_n \end{pmatrix} = \\
 \det \begin{pmatrix} x_1 & \ldots & x_{k-1} & y & x_{k+1} & \ldots  & x_n \end{pmatrix} +
 \det \begin{pmatrix} x_1 & \ldots & x_{k-1} & z & x_{k+1} & \ldots  & x_n \end{pmatrix}$
Außerdem: \\
$\det \begin{pmatrix} x_1 & \ldots & x_{k-1} & ay & x_{k+1} & \ldots  & x_n \end{pmatrix} =
a \det \begin{pmatrix} x_1 & \ldots & x_{k-1} & y & x_{k+1} & \ldots  & x_n \end{pmatrix}$

\skript{5.5/2} Wenn alle Spalten paarweise gleich sind, dann gilt $\det A = 0$

\skript{5.6a} Werden zwei Spalten von $A$ getauscht, so gilt: $\det A^\prime = - \det A$

\subsection{Laplace-Entwicklung}

Determinanten größerer Matrizen werden in der Praxis mit der Laplace-Entwicklung rekursiv berechnet. Diese sollte man per Hand aber wegen der Fehleranfälligkeit besser zu Gunsten der Vereinfachung und Verwendung obiger Sätze nicht verwenden.
Zuerst braucht man für die Laplace-Entwicklung die Definition des Minor:

\skript{5.11} Der Minor an der Stelle $(k,l)$ mit $k,l \in [1,n]$ von $A \in R^{n \times n}$ ist: \\
$$\textrm{Minor}_{k,l}(A) := \det \begin{pmatrix}
A_{1,1} & \ldots & A_{1,l-1} & A_{1,l+1} & \ldots & A_{1,n} \\
\vdots & & \vdots & \vdots & & \vdots \\
A_{k-1,1} & \ldots & A_{k-1,l-1} & A_{k-1,l+1} & \ldots & A_{k-1,n} \\
A_{k+1,1} & \ldots & A_{k+1,l-1} & A_{k+1,l+1} & \ldots & A_{k+1,n} \\
\vdots & & \vdots & \vdots & & \vdots \\
A_{n,1} & \ldots & A_{n,l-1} & A_{n,l+1} & \ldots & A_{n,n} \\
\end{pmatrix}$$

Es ist etwas schwer zu erkennen, aber der Minor an irgendeiner Stelle bedeutet ledglich: Berechne die Determinante der Matrix ohne Zeile $k$ und ohne Spalte $l$.

Die Laplace-Entwicklung ist dann möglich entweder nach Spalte $l$ oder nach Zeile $k$:

\skript{5.13a} Für $l \in [1,n]$ gilt: \\
$$\det A = \sum_{k=1}^n (-1)^{k+l} A_{k,l} \textrm{Minor}_{k,l}(A)$$

\skript{5.13b} Für $k \in [1,n]$ gilt: \\
$$\det A = \sum_{l=1}^n (-1)^{k+l} A_{k,l} \textrm{Minor}_{k,l}(A)$$

Da gilt es besonders hart drauf zu starren, bis sich einem das System ergibt. Oder einach weiterlesen, denn es folgt eine kurze Erklärung:

\begin{enumerate}
\setcounter{enumi}{-1}
\item \textit{(Pro-Tipp)} Vorher die Matrix mit elementaren Zeilenoperationen vereinfachen. Das verändert aber die Determinante etwas, siehe Korollar (5.6).
\item Eine Zeile oder Spalte aussuchen, nach der man entwickeln möchte. Mit Vorraussicht auf die nächsten Rechnungen sollte die Zeile/Spalte mit den meisten Nullen gewählt werden, das spart gleich Arbeit.
\item Jedes Element der gewählten Zeile/Spalte wird dann mit einem Minor multipliziert. Der Minor ist einfach die Determinante, wo die Entwicklungsspalte oder -zeile und die Zeile/Spalte des aktuellen Elementes weggestrichen wurden. Hier profitiert man von den Nullen.
\item Wenn die Minoren mit großen Matrizen zu berechnen sind, einfach rekursiv weiter Laplace-Entwickeln!
\item Die Ergebnisse der Minoren multipliziert mit dem Wert alternierend addieren. Ob man mit $(-1)$ oder $(+1)$ anfängt, hängt davon ab, welchen Index die Zeile/Spalte hat, nach der man entwickelt: Gerader Index $\implies (+1)$
\end{enumerate}

\subsection{Determinante eines Vektorraumendomorphismus}

Man beachte: Vektorraum\textbf{\underline{endo}}morphismus. \\

Die Determinante ist unabhängig von den Basen der Darstellungsmatrix:

\skript{5.9}Für  $\varphi : V \to V$, Basen $s = (s_1, \ldots, s_n), s^\prime = (s^\prime_1, \ldots, s^\prime_n)$ von $V$ gilt: \\
$\det \textrm{M}_{s,s}(\varphi) = \det \textrm{M}_{s^\prime, s^\prime}(\varphi)$ \\

Damit können wir auch problemlos $\det \textrm{M}_{s,s}(\varphi)$ mit $\det \varphi$ abkürzen.

\end{document}
