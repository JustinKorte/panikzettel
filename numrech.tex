\documentclass[a4paper,parskip=half*,DIV=15,fontsize=11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[english,ngerman]{babel}
\usepackage{braket}
\usepackage{listings}
\usepackage[pdftex,pdfencoding=auto,psdextra]{hyperref}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cool}
\usepackage{microtype}
\usepackage{lmodern}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclareMathOperator*\argmin{arg\,min}
\DeclareMathOperator\sign{sign}
\newcommand{\fl}{\mathrm{fl}}
\newcommand{\defeq}{\vcentcolon =}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\eps}{\mathrm{eps}}
\newcommand{\grad}{\nabla}

\lstset{
    mathescape=true,
    basicstyle=\ttfamily
}
\lstset{literate=%
    {Ö}{{\"O}}1
    {Ä}{{\"A}}1
    {Ü}{{\"U}}1
    {ß}{{\ss}}1
    {ü}{{\"u}}1
    {ä}{{\"a}}1
    {ö}{{\"o}}1
    {~}{{\textasciitilde}}1
}

\newcolumntype{C}{>{$}c<{$}}

\makeatletter
\AtBeginDocument{
  \hypersetup{
    pdftitle = {\@title},
    pdfauthor = {\@author}
  }
}
\makeatother

\title{Numerisches Rechnen Panikzettel}
\author{``der Dude'', Luca Oeljeklaus,\\ Tobias Polock, Philipp Schröer, Caspar Zecha}

\begin{document}

\maketitle

\begin{abstract}
Dieser Panikzettel ist über die Vorlesung Numerisches Rechnen. Er basiert auf dem Foliensatz von Prof. Dr. Martin Grepl aus dem Wintersemester 17/18.	\\
Dieser Panikzettel ist Open Source. Wir freuen uns über Anmerkungen und Verbesserungsvorschläge auf \url{https://git.rwth-aachen.de/philipp.schroer/panikzettel}.
\end{abstract}

\tableofcontents

\section{Fehleranalyse: Kondition, Rundungsfehler, Stabilität}

\subsection{Kondition}

Der \emph{absolute Fehler} in Daten $x$ und gestörten Daten $\tilde{x}$ ist $\norm{\Delta_x} := \norm{\tilde{x}-x}$.
Der \emph{relative Fehler} ist $\delta_x := \frac{\norm{\Delta_x}}{\norm{x}}$.

Für eine Funktion $f$ ist der Fehler in den Ausgabedaten $\tilde{y} = f(\tilde{x})$.

Mit \emph{Kondition} ist meist die \emph{relative Kondition} gemeint: $\frac{\delta_y}{\delta_x}$. Diese beschreibt die \emph{Sensitivität} des Problems bei Störungen in den Eingabedaten. Die \emph{absolute Kondition} ist $\frac{\norm{\Delta_y}}{\norm{\Delta_x}}$. Wir sagen, dass ein Problem \emph{gut konditioniert} ist, wenn seine Kondition klein ($\leq 1$) ist.

Als Approximation (über Taylorentwicklung 1.\ Ordnung) an die Kondition benutzen wir die \emph{Konditionszahl}.
\[\kappa_{\text{rel}}(x) := \abs*{f'(x)\frac{x}{f(x)}}\]

\subsection{Gleitpunktdarstellung}

Da die Menge der Reellen Zahlen nicht im Computer darstellbar ist${}^{\textrm{[Citation needed]}}$, approximieren wir diese durch Maschinenzahlen.
\[\mathbb{M}(b,m,r,R) = \Set{ \pm\sum_{i=1}^m d_{i}b^{e-i} | r \le e \le R,\ 0 \le d_i < b }\]

\begin{itemize}
  \item Relative Maschinengenauigkeit: $\eps := \frac{b^{1-m}}{2} = \inf \Set{ \delta > 0 | \fl(1 + \delta) > 1 }$
  \item Die betragsmäßig kleinste Zahl ($\neq 0$): $x_\mathrm{MIN} = b^{r-1}$
  \item Die betragsmäßig größte Zahl: $x_\mathrm{MAX} = (1 - b^{-m}) b^R$
\end{itemize}

Für IEEE Double-precision floating point-Zahlen ist $\eps = 2.2204 \times 10^{-16}$.

Die Abbildung $\fl : \mathbb{R} \to \mathbb{M}(b,m,r,R)$ rundet. Es gilt $\fl(x) = x (1 + \delta)$ für ein $\abs \delta \leq \eps$.

Für Multiplikation und Division von Maschinenzahlen ist $\kappa_\text{rel} \leq 1$. Diese Operationen sind also immer gut konditioniert. Bei Addition und Subtraktion gilt dagegen $\kappa_\text{rel} \gg 1$, wenn das Ergebnis gegen Null geht.

\subsection{Stabilität}

\emph{Stabile} Algorithmen erzeugen Fehler nur in der Größenordnung des Fehlers, der durch die Kondition des Problems unvermeidbar ist.

Ein \emph{rückwärts stabiles} Verfahren $\tilde{f}$, das eine Funktion $f$ approximiert, gibt die exakte Antwort auf eine nahezu richtige Frage: Für alle $x \in X$ gilt
\[ \tilde{f}(x) = f(\tilde{x}) \text{ für ein } \tilde{x} \text{ mit } \frac{\norm{x-\tilde{x}}}{\norm{x}} = \mathcal{O}(\eps) \]

Dann wissen wir: $\frac{\norm{\tilde{f}(x) - f(x)}}{\norm{f(x)}} = \mathcal{O}(\kappa(x) \eps)$.

\subsection{Taylorentwicklung}

Funktionen lassen sich als die Taylorentwicklung darstellen. Für ein $\xi$ zwischen $x$ und $x_0$ ist
\[f(x) = \sum_{i=0}^{n-1} \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i + \frac{f^{(n)}(\xi)}{n!}(x-x_0)^n\]

Somit lässt sich eine Funktion $f$ durch das sogenannte Taylorpolynom $n$-ten Grades approximieren:
\[p_n(x) = \sum_{i=0}^{n} \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i\]

\section{Lineare Gleichungssysteme, direkte Lösungsverfahren}

\subsection{Kondition und Störungssätze}

Díe \emph{Konditionszahl einer Matrix} ist $\kappa(A)_{\norm{\cdot}} = \norm{A} \norm{A^{-1}}$ bzgl.\ der $\norm{\cdot}$-Norm.

Für ein lineares Gleichungssystem $Ax = b$ mit Störung in $b$ bzw. in $A$ und $b$ gilt:
\begin{align*}
	\frac{\norm{\Delta x}}{\norm{x}} &\leq \kappa(A) \frac{\norm{\Delta b}}{\norm{b}}	\\
    \frac{\norm{\Delta x}}{\norm{x}} &\leq \frac{\kappa(A)}{1-\kappa(A)\frac{\norm{\Delta A}}{\norm{A}}}\left(\frac{\norm{\Delta A}}{\norm{A}}+\frac{\norm{\Delta b}}{\norm{b}}\right)
\end{align*}

\subsection[LR-Zerlegung]{$LR$-Zerlegung (Gauß-Elimination)}

Bei der Gauß-Elimination wird eine Matrix und die zugehörige rechte Seite durch elementare Zeilentransformationen in rechte obere Dreiecksform gebracht, die anschließend durch Rückwärtseinsetzen einfach gelöst werden kann.

Bei der \emph{LR-Zerlegung} werden diese Zeilentransformationen zusätzlich in einer Matrix $L$ gespeichert. So wird $A \in \mathbb{R}^{n \times n}$ in $A = LR$ zerlegt.
$L$ ist eine normierte linke untere und $R$ eine rechte obere Dreiecksmatrix.

Etwa ist $L_{2,1} = \frac{A_{2,1}}{A_{1,1}}$, also der Faktor mit dem man Zeile 1 von Zeile 2 subtrahiert, um $A_{2,1}$ zu eliminieren.

So können wir $Ax = b$ über $L y = b$ und $Rx = y$ lösen.

Um die Kondition des Problems zu verbessern, können wir mit Diagonalmatrix $D_Z$ \emph{äquilibieren}. Wir teilen jede Zeile durch ihre Betragssumme. Das gibt uns die minimale Konditionszahl bezüglich der $\infty$-Norm.

Die \emph{LR-Zerlegung mit Spaltenpivotisierung} ist eine Zerlegung $PA = LR$ mit einer Permutationsmatrix $P$. $P$ entsteht, indem in jeder Spalte das betragsgrößte Element als \emph{Pivotelement} wählt und dafür die Matrixzeilen vertauscht. Dies ist nötig, da sonst das Pivotelement verschwinden könnte (Division durch null!) und es ist sowieso gut für Stabilität.

\subsection[Cholesky-Zerlegung]{Cholesky-Zerlegung ($LDL^T$-Zerlegung)}

\begin{minipage}[t]{0.5\textwidth}
Für symmetrisch positiv definite Matrix $A \in \mathbb{R}^{n \times n}$ existiert eine Zerlegung
\[A = LDL^T\]
wobei $L$ eine normierte linke untere Dreiecksmatrix und $D$ eine Diagonalmatrix ist.
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vspace{-2em}
\begin{align*}
  d_{k,k} &= a_{k,k} - \sum_{j=1}^{k-1} l_{k,j}^2 d_{j,j} \\
  l_{i,k} &= \left( a_{i,k} - \sum_{j=1}^{k-1} l_{i,j} d_{j,j} l_{k,j} \right) / d_{k,k}
\end{align*}
\end{minipage}

Zur Berechnung läuft man einfach durch die Spalten und berechnet $d_{1,1}, l_{2,1}, \ldots$. Auch cool: Ist $d_{i,i} > 0$ für alle $i$ und $L$ normierte Dreiecksmatrix, dann ist $A$ symmetrisch positiv definit! Cholesky kann also erst angewandt werden und dann das Ergebnis auf Gültigkeit geprüft werden.

Für $n = 3$ ergibt sich beispielsweise
\begin{align*}
LDL^T &= \begin{pmatrix}
	1	\\
    l_{2,1}	&	1	\\
    l_{3,1}	&	l_{3,2}	&	1
\end{pmatrix} \begin{pmatrix}
	d_{1,1}	\\
    	&	d_{2,2}	\\
        &	&	d_{3,3}
\end{pmatrix} \begin{pmatrix}
	1	&	l_{2,1}	&	l_{3,1}	\\
    	&	1	&	l_{3,2}	\\
    	&	&	1
\end{pmatrix}	\\
&= \begin{pmatrix}
	d_{1,1}	&	l_{2,1}d_{1,1}	&	l_{3,1}d_{1,1}	\\
    l_{2,1}d_{1,1}	&	l_{2,1}^2d_{1,1} + d_{2,2}	&	l_{2,1}l_{3,1}d_{1,1}+l_{3,2}d_{2,2}	\\
    l_{3,1}d_{1,1}	&	l_{2,1}l_{3,1}d_{1,1}+l_{3,2}d_{2,2}	&	l_{3,1}^2d_{1,1} + l_{3,2}^2d_{2,2} + d_{3,3}
\end{pmatrix}
\end{align*}

\subsection[QR-Zerlegung]{$QR$-Zerlegung}

Bei der $QR$-Zerlegung wird eine Matrix $A$ durch orthogonale Transformationen $Q^T$ in rechte obere Dreiecksform gebracht. Rechte obere Dreiecksform bedeutet, dass die ersten $k$ Spalten von $R$ den selben Raum wie die ersten $k$ Einheitsvektoren aufspannen. Es geht also bei der $QR$-Zerlegung darum, durch orthogonale Transformationen den Raum, der von den ersten $k$ Spalten von $A$ aufgespannt wird, in die von den ersten $k$ Einheitvektoren aufgespannte Hyperebene abzubilden.

Da orthogonale Transformationen genau Kombinationen von Rotationen und Spiegelungen sind, ergeben sich folgende drei Optionen für die Transformationen.

\subsubsection{Gram-Schmidt-Orthogonalisierung}

Ausführlich erklärt in unserem \href{http://panikzettel.philworld.de/la.pdf#page=20}{Panikzettel zu Lineare Algebra}. Dieses Verfahren ist nicht stabil.

\subsubsection{Givens-Rotation}

Die Givens-Rotationsmatrix $G_{i,k}$ rotiert in der $x_i$-$x_k$-Ebene die Matrix $A$ so, dass in $G_{i,k} A$ der Eintrag an Position $i, k$ Null wird.

$G_{i,i} = c = G_{k,k}$, $G_{i,k} = s$ und $G_{k,i} = -s$. Dazu Einsen auf die Diagonale und sonst Nullen.
\begin{minipage}{0.7\textwidth}
\[
  G_{i,k} = \left(\begin{array}{ccccccccccc}
    1 \\
    & \ddots  & & & & & & & & 0 \\
    & & 1 \\
    & & & c & 0 & \cdots  & 0 & s \\
    & & & 0 & 1 & & & 0 \\
    & & & \vdots  & & \ddots  & & \vdots  \\
    & & & 0 & & & 1 & 0 \\
    & & & -s  & 0 & \cdots  & 0 & c \\
    & & & & & & & & 1 \\
    & 0 & & & & & & & & \ddots  \\
    & & & & & & & & & & 1
  \end{array}\right)
\]
\end{minipage}
\begin{minipage}{0.3\textwidth}
\begin{align*}
   r &= \pm\norm*{\left(\begin{array}{c}
   A_{i,i}  \\  A_{i,k}
   \end{array}\right)}_2  \\
   c &= \frac{A_{i,i}}{r} \\
   s &= \frac{A_{i,k}}{r}
\end{align*}
\end{minipage}

Wir wenden also diese Rotationsmatrizen auf alle Elemente unterhalb der Diagonalen von $A$ an, sodass wir eine QR-Zerlegung für $A$ erhalten:
\[ R = \underbrace{\ldots G_{2,3} \ldots G_{1,2}}_{Q^T} A \text{ bzw. }  A = \underbrace{G_{1,2}^T \ldots G_{2,3}^T \ldots}_{Q} R \]

$Q$ wird nie explizit berechnet. Zur Lösung von $QRx = b$: Berechne $y = \ldots G_{2,3} \ldots G_{1,2} b$ und löse $Rx = y$ durch Rückwärtseinsetzen.

\subsubsection{Householder-Transformation}

Die Householder-Transformation entspricht Spiegelungen. Die Householder-Matrix $Q_v$ spiegelt an der zu $v$ orthogonalen Hyperebene ($v$ ist Normalenvektor).
\[ Q_v = I - 2 \frac{v v^T}{v^T v} \]

$v v^T$ ist eine Dyade. $v^T v$ ist ein Skalar zur richtigen Skalierung. Die 2 taucht auf, weil man ja nicht genau einmal den Normalenvektor in die Hyperebene, sondern genau zwei mal den Normalenvektor durch (auf die andere Seite) der Hyperebene will.

Die folgenden Eigenschaften sind schön zu beweisen:

\begin{minipage}[t]{0.5\textwidth}
\begin{itemize}
  \item Orthogonalität: $Q_v^{-1} = Q_v^T$.
  \item Selbstinverse: $Q_v^2 = I$.\\Zweimalige Spiegelung ergibt den ursprünglichen Punkt.
  \item $Q_{\alpha v} = Q_v, \alpha \in \mathbb{R} \setminus \{0\}$.\\Skalierung des Normalenvektors ändert nicht die Spiegelebene.
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{itemize}
  \item $Q_v y = y \Leftrightarrow y^T v = 0$.\\
        Ursprünglicher und gespiegelter Punkt sind nur identisch, wenn der Punkt in der Spiegelebene liegt.
  \item $Q_v v = -v$.\\
        Spiegelung des Normalenvektors vertauscht das Vorzeichen.
\end{itemize}
\end{minipage}

Mit Householder schauen wir uns immer die erste Spalte $a_1$ an und spiegeln die auf die Ebene zum Einheitsvektor $e_1$. Danach wiederholen wir das Gleiche auf der Teilmatrix $A$ ohne erste Zeile und Spalte (in den $Q_v$ fügen wir passen Nullen ein).
\[v = a_1 + \sign(a_{1,1}) \norm{a_1} e_1\]

Mit $Q_i$ als $Q_v$ mit $v$ für Teilmatrix $i$ ist dann die QR-Zerlegung:
\[Q_{n-1} \ldots Q_2 Q_1 A = R \text{ bzw. } A = Q_1^T Q_2^T \ldots Q_{n-1}^T R = QR\]

Übrigens ist allgemeiner $v = a_1 \pm \norm{a_1} e_1$ möglich, aber wir möchten Auslöschung vermeiden.

\section{Lineare Ausgleichsrechnung}

Bei einem \emph{Linearen Ausgleichsproblem} wollen wir ein überbestimmtes LGS so lösen, dass der Fehler möglichst klein ist. Für $A \in \mathbb{R}^{n \times m}$ und $b \in \mathbb{R}^n$ mit $m < n$ suchen wir $x^\ast$:
\[x^\ast = \argmin_{x \in \mathbb{R}^{m}} \norm*{Ax-b}\]

Im Folgenden werden wir annehmen, dass es sich bei der gewählten Norm um die $2$-Norm handelt, da das Problem sonst nicht eindeutig lösbar ist.

Geometrische Vorstellung: Wir wollen $x^\ast$ so, dass das Residuum $r = b - Ax^\ast$ orthogonal auf $Ax^\ast$ steht, denn dann wird $r$ am kleinsten.

Für die Kondition des Problems gilt mit $\cos \Theta = \frac{\norm{A x^\ast}_2}{\norm{b}_2}$ und $\kappa_2(A) = \frac{\max_{\norm{x} = 1} \norm{Ax}_2}{\min_{\norm{x} = 1} \norm{Ax}_2}$:
\begin{align*}
    \frac{\norm{\tilde x - x^\ast}}{\norm{x^\ast}} &\leq \frac{\kappa_2(A)}{\cos \Theta} \frac{\norm{\tilde b - b}_2}{\norm{b}_2} \\
    \frac{\norm{\tilde x - x^\ast}}{\norm{x^\ast}} &\leq (\kappa_2(A) + \kappa_2(A)^2 \tan \Theta) \frac{\norm{\tilde A - A}_2}{\norm{A}_2}
\end{align*}

\subsection{Normalengleichungen}

Mit der Lösung von \emph{Normalengleichungen} lässt sich das Problem auf einfache LGS zurückführen:
\[\left(A^TA\right)x^\ast = A^Tb\]

Es gibt immer eine Lösung. Sie ist genau dann eindeutig, wenn $A$ vollen Rang hat.

Ausgleichsrechnung über Normalengleichungen ist allerdings numerisch instabil.

\subsection[QR-Zerlegung]{$QR$-Zerlegung}

Ein numerisch stabiles Verfahren zur Lösung des Linearen Ausgleichsproblems ergibt sich mithilfe der $QR$-Zerlegung.

Sei eine QR-Zerlegung gegeben mit $Q \in \mathbb{R}^{n \times n}$ und $R = \begin{pmatrix} \hat{R} \\ 0 \end{pmatrix} \in \mathbb{R}^{n \times m}$, wobei $\hat{R} \in \mathbb{R}^{m \times m}$. Weil orthogonale Projektionen die 2-Norm nicht ändern, gilt:
\[
  x^\ast = \argmin_{x \in \mathbb{R}^{m}} \norm*{Ax-b}_2 = \argmin_{x \in \mathbb{R}^m} \norm*{Rx-Q^Tb}_2
\]

Berechnen wir $\hat{b} \in \mathbb{R}^n$ mit $Q^T b = \begin{pmatrix} \hat{b} \\ r \end{pmatrix}$, können wir $\hat{R}x^\ast = \hat{b}$  durch Rückwärtseinsetzen lösen. Dann ist auch die Norm des Residuums durch $\norm{r}_2$ gegeben.

\subsection{Singulärwertzerlegung (SVD)}

Die \emph{Singulärwertzerlegung} einer Matrix $A \in \mathbb{R}^{n \times m}$ ist
\[A = U\Sigma V^T\]
mit orthogonalen Matrizen $U \in \mathbb{R}^{n \times n}, V \in \mathbb{R}^{m \times m}$ und einer Diagonalmatrix $\Sigma \in \mathbb{R}^{n \times m}$. Die nicht-null Einträge von $\Sigma$ heißen Singulärwerte von $A$, die Spalten von $U$ heißen Linkssingulärvektoren, die Spalten von $V$ heißen Rechstsingulärvektoren.

SVD gibt die Pseudoinverse $A^+ := V\Sigma^+ U^T$ mit $\Sigma^+$ der Diagonalmatrix, die die invertierten Singulärwerte von $A$ enthält.

Auch damit können wir das Lineare Ausgleichsproblem stabil lösen:
\[ x^\ast = A^+ b \implies x^\ast = \argmin_{x \in \mathbb{R}^m} \norm*{Ax-b}\]
Sind die Singulärvektoren nach betragsmäßiger Größe geordnet, ergibt sich die Lösung mit kleinster $2$-Norm.

\section{Nichtlineare Gleichungssysteme}

\subsection{Fixpunktiteration}

Für eine Funktion $\Phi: \mathbb{R}^n \to \mathbb{R}^n$ heißt $x \in \mathbb{R}^n$ Fixpunkt von $\Phi$ gdw. $\Phi(x) = x$.

Damit die Fixpunktiteration auf $\Phi$ funktioniert, muss gelten:
\begin{itemize}
  \item Selbstabbildung auf einem $E \subseteq \mathbb{R}^n$: $\Phi : E \to E$.
  \item Kontraktion: $\norm{\Phi(x) - \Phi(y)} \leq L \norm{x-y}$ für alle $x,y \in E$ mit $L < 1$.
\end{itemize}

Nach dem \emph{Banachschen Fixpunktsatz} gilt dann: $\Phi$ hat einen eindeutigen Fixpunkt $x^\ast \in E$ und die Fixpunktiteration $x^{i+1} = \Phi(x^i)$ konvergiert für alle $x^0 \in E$.

Fehlerabschätzungen:
\begin{alignat*}{3}
	\norm{x^k-x^\ast} &\le \frac{L^k}{1-L} \norm{x^1-x^0} &\mbox{\small $\quad\text{bzw. } \log \left( (1-L) \frac{\norm{x_k-x^\ast}}{\norm{x^1-x^0}} \right) / \log(L) \le k$}	\quad&&	\text{\emph{(a priori)}}	\\
	\norm{x^k-x^\ast} &\le \frac{L}{1-L} \norm{x^k-x^{k-1}}	& &&	\text{\emph{(a posteriori)}}
\end{alignat*}

Das Nullstellenproblem $f(x^\ast) = 0$ ist äquivalent zum Fixpunktproblem
\[ x^\ast = \Phi(x^\ast) \quad\text{mit}\quad \Phi(x) := x - M_{x^\ast} f(x) \]
und mit einer Matrix $M_{x^\ast}$, die in einer Umgebung von $x^\ast$ invertierbar ist.

\subsection{Newton-Verfahren}

Es sei $F : \mathbb{R}^n \to \mathbb{R}^n$ stetig differenzierbar. Beim \emph{(klassischen) Newton-Verfahren} lösen wir in jedem Schritt:
\[ F'(x^k) \underbrace{(x^{k+1} - x^k)}_{:= s^k} = -F(x^k) \quad\text{bzw.}\quad x^{k+1} = x_k - (F'(x^k))^{-1} \cdot F(x^k) \]
$s^k$ ist die Schrittweite ($x^{k+1} = x^k + s^k$). Das Newton-Verfahren konvergiert lokal quadratisch.

Das \emph{vereinfachte Newton-Verfahren} verwndet statt $F'(x^k)$ einfach $F'(x^0)$ in jedem Schritt, d.h. die Iteration ist gegeben durch:
\[ x^{k+1} = x_k - (F'(x^0))^{-1} \cdot F(x^k) \]
Dabei geht quadratische Konvergenz verloren, aber  über LR-Zerlegung gewinnen wir Effizienz (in $F'(x_0) (x^{k+1} - x^k) = -F(x^k)$ wiederholen sich linke Seiten).

\section{Nichtlineare Ausgleichsrechnung}

Beim \emph{nichtlinearen Ausgleichsproblem} sucht man für eine (nicht unbedingt lineare) Funktion $F : \mathbb{R}^n \to \mathbb{R}^m$ ein $x^\ast$ sodass
\[ x^\ast = \argmin_{x \in \mathbb{R}^n} \norm{F(x)}_2 \]

\subsection{Gauß-Newton-Verfahren}

Die lineare Approximation von $F(x)$ ist $F(x^k) + F'(x^k)(x-x^k)$ (Taylor-Entwicklung).

Das \emph{Gauß-Newton-Verfahren} ist eine Fixpunktiteration, bei der $F(x)$ linear approximiert wird:
\[ x^{k+1} = \arg \min_{x \in \mathbb{R}^n} \norm{ \underbrace{F(x^k)}_{\widehat{=} -b} + \underbrace{F'(x^k)}_{\widehat{=} A} \underbrace{(x-x^k)}_{\widehat{=} s^k} }_2 \]
In jedem Schritt wird also ein lineares Ausgleichsproblem $\arg\min_{s^k \in \mathbb{R}^n} \norm{A s^k - b}_2$ gelöst.

Wenn $F'(x)$ vollen Rang hat, können wir auch schreiben (über Normalengleichung):
\[ x^{k+1} = x^k - (F'(x^k)^T F'(x^k))^{-1} \cdot F'(x^k)^T F(x^k) \]

\subsection{Bonus: Newton-Verfahren}

Eigentlich nur hier zum Vergleich der Gauß-Newton-Fixpunktiteration mit der Newton-Iteration.

Wir wissen, dass gilt:
\[ x^\ast = \argmin_{x \in \mathbb{R}^n} \norm{F(x)}_2 = \argmin_{x \in \mathbb{R}^n} \frac{1}{2} \norm{F(x)}_2^2 = \argmin_{x \in \mathbb{R}^n} \frac{1}{2} F(x)^T \cdot F(x) \]

$\phi(x) := \frac{1}{2} F(x)^T \cdot F(x)$ hat an einer Stelle $x^\ast$ ein lokales Minimum genau wenn $\grad\phi(x^\ast) = 0$ und wenn $\phi''(x^\ast)$ symmetrisch positiv definit ist.
Also könnte man mit dem Newton-Verfahren eine Nullstelle von $\grad \phi(x)$ bestimmen:
\begin{align*}
  x^{k+1} &= x^k - (\phi''(x^k))^{-1} \grad \phi(x^k) \\
          &= x^k - (F'(x^k)^T F'(x^k) + \underbrace{\sum_{i=1}^m F_i(x^k) F_i''(x^k))^{-1} \grad \phi(x^k)}_\text{hässliche Summe}
\end{align*}
Die hässliche Summe wird beim Gauß-Newton-Verfahren ignoriert.

\subsection{Levenberg-Marquardt-Verfahren}

Das \emph{Levenberg-Marquardt-Verfahren} ist eine Modifikation des Gauß-Newton-Verfahrens, bei dem bei der Berechnung der Schrittweite $s^k = x^{k+1} - x^k$ ein Dämpfungsfaktor $\mu > 0$ eingefügt wird:
\[ s^k = \argmin (\norm{F'(x^k)s^k + F(x^k)}^2_2 + \mu^2 \norm{s^k}^2_2 = \argmin_{s \in \mathbb{R}^n} \norm{\underbrace{\begin{pmatrix} F'(x^k) \\ \mu I \end{pmatrix}}_{\substack{\text{immer voller}\\\text{Rang}}} s + \begin{pmatrix} F(x^k) \\ 0 \end{pmatrix}}_2 \]

Als Fixpunktiteration:
\[ x^{k+1} = x^k - (F'(x^k)^T F'(x^k) + \mu^2 I)^{-1} \cdot F'(x^k) F(x^k) \]

\section{Interpolation}
Den Raum der Polynome vom Grad $n$ bezeichnen wir als $\Pi_n$, und deren Elemente als $P_n (x)$. Wir haben Daten (z.B. Messwerte) $(x_i,f(x_i) = y_i),\, i \in \{0,\ldots,n\}$ gegeben und wollen diese durch ein Polynom $P_n(x)$ darstellen: Es soll also $P_n(x_i) = y_i$ für alle $i$ gelten. Ein solches Polynom existiert immer eindeutig.

\subsection{Lagrange-Polynominterpolation}
Die \emph{Lagrange-Fundamentalpolynome} $\ell_i(x)$ bilden eine Orthogonalbasis von $\Pi_n$:
\[
  \ell_i(x) := \prod_{\substack{k = 0 \\ k \neq i}}^n \frac{x - x_k}{x_i - x_k},
  \quad\text{dafür gilt}\quad
  \ell_i(x_j) = \delta_{ij} = \begin{cases}
    1, &\text{falls } i = j \\
    0, &\text{falls } i \neq j
  \end{cases}
\]

Dann ist das \emph{Lagrange-Interpolationspolynom}:
\[ P_n(f|x_0,\ldots, x_n) := \sum_{j=0}^n f (x_j) \ell_j = P_n(x) \]

\subsection{Auswertung des Polynoms durch das Neville-Aitken-Schema}
Wir wollen nun den Wert unseres Interpolationspolynoms an \emph{einer festen Stelle} $x$ auswerten, ohne das Polynom explizit aufzustellen und dann erst auszuwerten.

Wir definieren:
\[ P_{i,k} := P(f|x_{i-k},\ldots,x_i)(x), \]
also insbesondere
\[ P_{i,0} = P(f|x_i)(x) = f(x_i) \quad\text{und}\quad P_{n,n} = P(f|x_0,\ldots,x_n)(x) \]

Das \emph{Neville-Aitken-Schema} berechnet $P_n(f|x_0,\ldots, x_n)(x) = P_{n,n}$ rekursiv:

\begin{minipage}{0.6\textwidth}
\begin{align*}
  P_{i,0} &= f(x_i) \\
  P_{i,k} &= P_{i,k-1} + \frac{x - x_i}{x_i - x_{i-k}} (P_{i,k-1} - P_{i-1,k-1})
\end{align*}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{tabular}{C|CCCC}
& P_{i,0} & P_{i,1} & P_{i,2} & \ldots \\
\hline
x_0 & f(x_0) \\
x_1 & f(x_1) & P_{1,1} \\
x_2 & f(x_2) & P_{2,1} & P_{2,2} \\
\vdots & \vdots \\
x_n  & f(x_n) & \ldots & \ldots  & P_{n,n}
\end{tabular}
\end{minipage}

\subsection{Berechnung der Potenzform über ein LGS}
Wir können das Interpolationspolynom $P_n(x)$ auch über das folgende LGS bestimmen, welches wegen der Vandermode-Matrix im Allgemeinen lächerlich schlecht konditioniert ist:

\[\underbrace{\begin{pmatrix} 1 & x_0 & x_0^2 & \cdots & x_0^n \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \ldots & x_n^n\end{pmatrix}}_{\text{Vandermonde-Matrix $V_n$}} \begin{pmatrix} a_0 \\ \vdots \\ a_n \end{pmatrix} = \begin{pmatrix} y_0 \\ \vdots \\ y_n \end{pmatrix}\]

\subsection{Auswertung der Potenzform mit dem Horner-Schema}
Falls die Potenzform des Polynoms $P_n (x)$ vorliegt, kann diese durch geschickte Klammerung schnell (etwa halber Rechenaufwand) für ein $x$ ausgewertet werden.

\[ a_0 + a_1x + \hdots + a_nx^n = a_0 + x\cdot (a_1 + x\cdot (a_2 + (\hdots  x\cdot (a_{n_1} + xa_n) \hdots) ) ) \]

\subsection{Newtonsche Interpolationsformel}
Sei $P(f|x_0,\ldots,x_{n-1})$ gegeben und wir wollen nun mit möglichst wenig Aufwand $P(f|x_0,\ldots,x_n)$ berechnen.

Es gilt:
\[P(f|x_0,\ldots,x_n)(x) = P(f|x_0,\ldots,x_{n-1}) (x) + \underbrace{\delta_n(x-x_0)\cdots (x-x_{n-1})}_\textit{Korrekturterm}\]
mit $\delta_n$ der Koeffizient der höchsten Potenz $x^n$, bzw. $[x_0,\ldots,x_n]f$ weil der Koeffizient eben von $x_0, \ldots, x_n$ abhängt:
\[\ \delta_n \eqdef [x_0,\ldots,x_n]f \eqdef \frac{f(x_n) - P(f|x_0,\ldots,x_{n-1})(x_n)}{(x_n -x_0)\ldots (x_n-x_{n-1})} \]

So lässt sich auch von einer einzigen Stützstelle (z.B. $P(f|x_0)$) aus das gesamte Interpolationspolynom $P(f|x_0,\ldots,x_n)$ rekursiv bestimmen. Wir erhalten die \emph{Newtonsche Interpolationsformel}:
\begin{align*}
  P(f|x_0,\ldots,x_n)(x) &= [x_0]f + (x-x_0)\cdot [x_0,x_1]f \\
                         &+ (x-x_0)\cdots(x-x_1)\cdot [x_0,x_1,x_2]f \\
                         &+ \ldots \\
                         &+ (x-x_0)\cdots(x-x_{n-1})\cdot [x_0,\ldots,x_n]f
\end{align*}

Die sich wiederholenden Polynome $\omega_i(x)$ bilden die \emph{Newtonsche Basis} von $\Pi_n$:
\[ \omega_0 \defeq 1,\quad\quad \omega_1 \defeq (x-x_0),\quad\quad \ldots,\quad\quad \omega_n \defeq (x-x_0)\ldots (x-x_{n-1}) \]

Für die Newtonsche Interpolationsformel haben wir das rekursive \emph{Schema der dividierten Differenzen} für verschiedene Stützstellen $x_0,\ldots,x_n$:

\begin{minipage}{0.4\textwidth}
\[ [x_0,\ldots,x_n]f = \frac{[x_1,\ldots, x_n]f - [x_0,\ldots, x_{n-1}]f}{x_n - x_0} \]
\end{minipage}\hspace{0.1\textwidth}
\begin{minipage}{0.5\textwidth}
\setlength{\tabcolsep}{0.5em}
\renewcommand{\arraystretch}{0.4}
\small
\begin{tabular}{C|CCCCCC}
& [x_i]f & & [x_i,x_{i+1}]f & & [x_i,x_{i+1},x_{i+2}]f \\
\hline
x_0 & [x_0]f \\
    & &> & [x_0,x_1]f \\
x_1 & [x_1]f & & &> &[x_0,x_1,x_2]f \\
    & &> & [x_1,x_2]f \\
x_2 & [x_2]f
\end{tabular}
\end{minipage}

\subsection{Fehlerabschätzung}
Der Fehler der Polynominterpolation $P_n (f|x_0,\ldots,x_n) (x)$ zur interpolierten Funktion $f(x)$, auf dem Intervall $[a,b]$, kann abgeschätzt werden durch:

\[\abs*{f(x) - P_n (x)} \leq \abs*{\omega_{n+1}} \max_{\xi \in [a,b]} \frac{\abs*{f^{(n+1)}(\xi)}}{(n+1)!}\]

\section{Numerische Differentiation}

Wir reduzieren die numerische Differentiation auf Polynominterpolation:
Die abzuleitende Funktion wird erst durch ein Polynom interpoliert.
Das Interpolationspolynom kann dann leicht abgeleitet werden.
Die $n$-te Ableitung eines Polynoms ist $n! \cdot (\text{Koeffizient } n)$.
Bei der Newton-Interpolationsformel ist dieser $n$-te Koeffizient genau $[x_0,\ldots,x_n]f$.
Wir erhalten:
\[ f^{(n)}(x) \quad\approx\quad P(f|x_0, \ldots, x_n)^{(n)} (x) = n! \cdot [x_0,\ldots,x_n]f \]

Spezialfälle für äquidistante Stützstellen $x_j = x_0 + j \cdot h$ und $x \in [x_0, x_n]$:
\begin{align*}
  f'(x) &\approx [x_0,x_1]f = \frac{f(x_1) - f(x_0)}{h} \\
  f''(x) &\approx 2! \cdot [x_0,x_1,x_2]f \\
         &= \frac{f(x_2) - 2 \cdot f(x_1) + f(x_0)}{h^2}
\end{align*}

Fehler $\varepsilon$ in den Daten, $\abs{f(y) - \tilde{f}(y)} \leq \varepsilon$, können zu Auslöschung führen.
Für die zweite Ableitung mit äquidistanten Stützstellen mit Differenzenquotienten:
\[ \abs{\tilde{f}'' - f''(x)} \leq \underbrace{\frac{4 \cdot \varepsilon}{h^2}}_\text{Rundungsfehler} + \underbrace{c \cdot h^2}_\text{Diskretisierungsfehler} \]
Die Fehlerschranke wird für $h = \sqrt[4]{4 \cdot \varepsilon/c}$ minimal.

\section{Numerische Integration}

Der Ansatz bei der numerischen Integration einfach:
Das Integral $\Int{f(x)}{x,a,b}$ wird in Teilstücke aufgeteilt und dann werden diese Teilintegrale mithilfe einer einfach integrierbaren Funktion approximiert.

Formal teilen wir also das Intervall $[a, b]$ in Teilintervalle $[t_{k-1}, t_k]$ auf und approximieren $f$ durch ein $g_k$ für jedes Intervall:
\[
  \Int{f(x)}{x,a,b} = \sum_{k=1}^n \Int{f(x)}{x,t_{k-1},t_k} \approx \sum_{k=1}^n \Int{g_k(x)}{x,t_{k-1},t_k}
\]

\subsection{Kondition}

Zunächst die Kondition der Integration mit Approximation.
Wir approximieren $f$ durch $\tilde{f}$.
\[
  I = \Int{f(x)}{x,a,b} \qquad\qquad \tilde{I} = \Int{\tilde{f}(x)}{x,a,b}
\]

Dann gilt: $\abs{I - \tilde{I}} \leq (b - a) \norm{f - \tilde{f}}_\infty$. Also ist die Kondition:
\[
  \frac{\abs{I - \tilde{I}}}{\abs{I}} \leq \underbrace{\frac{\int_a^b \norm{f}_\infty dx}{\norm{\int_a^b f(x) dx}}}_{\kappa_\mathrm{rel}} \cdot \frac{\norm{f - \tilde{f}}_\infty}{\norm{f}_\infty}
\]

\subsection{Quadraturformeln}

Eine Approximation einer Funktion $f$ mit Stellen $x_0, \ldots, x_m \in [c, d]$ zur Integration wird \emph{Quadraturformel} genannt:
\[ I_m(f) := \Int{P(f \vert x_0, \ldots, x_m)(x)}{x,c,d} \]

Ist $f$ ein Polynom mit Grad $\leq m$, dann gibt die Quadraturformel ein exaktes Ergebnis.

In die Quadraturformeln wird nun die Definition des Lagrange-Integrationspolynoms eingesetzt.
Ein wenig Umformen ergibt die folgenden Formeln. Sei dazu $h = d - c$.
\begin{align*}
  I_m(f) = h \sum_{j=0}^m c_j f(x_j) && c_j := \frac{1}{h} \int_c^d \underbrace{\prod_{\substack{k=0 \\ k\neq j}}^m \frac{x - x_k}{x_j - x_k}}_{\ell_{jm}(x)} dx
\end{align*}

Die Lagrange-Fundamentalpolynome $\ell_{jm}$ sind nur von den Stützstellen, aber nicht von den Funktionswerten abhängig.

\subsection{Newton-Cotes-Formeln}

Wir setzen die Stützstellen äquidistant mit der Form:
\begin{center}
  \begin{tabular}{cc}
    $m = 0$ & $m > 0$ \\
    $x_0 = c + \frac{1}{2} h$ & $x_j = c + \frac{j}{m} h$
  \end{tabular}
\end{center}

Man erhält dann die \emph{Newton-Cotes-Formeln} mit normierten Stützstellen $\xi_j$ und Gewichten $c_j$. Die Gewichte sind unabhängig vom Intervall $[c, d]$.
\[ I_m(f) = h \sum_{j=0}^m c_j f(c + \xi_j h) \]

\begin{center}
  \renewcommand{\arraystretch}{1.7}
  \begin{tabular}{|C|c|C|C|C|}
  \hline
  m	&	&	\xi_j	&	c_j	&	I_m(f) - \int_c^d f(x) dx	\\\hline
  0	&	Mittelpunktsregel	&	\frac{1}{2}	&	1	&	-\frac{1}{24}h^3 f^{(2)}(\xi)	\\
  1	&	Trapezregel	&	0, 1	&	\frac{1}{2}, \frac{1}{2}	&	\frac{1}{12}h^3 f^{(2)}(\xi)	\\
  2	&	Simpson-Regel	&	0, \frac{1}{2}, 1	&	\frac{1}{6}, \frac{4}{6}, \frac{1}{6}	&	\frac{1}{90} \left ( \frac{1}{2} h \right)^5 f^{(4)}(\xi)	\\
  3	&	$\frac{3}{8}$-Regel	&	0, \frac{1}{3}, \frac{2}{3},~1	&	\frac{1}{8}, \frac{3}{8}, \frac{3}{8}, \frac{1}{8}	&	\frac{3}{80} \left( \frac{1}{3} h \right)^5 f^{(4)}(\xi)	\\
  4	&	Milne-Regel	&	0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1	&	\frac{7}{90}, \frac{32}{90}, \frac{12}{90}, \frac{32}{90}, \frac{7}{90}	&	\frac{8}{945}\left(\frac{1}{4}h\right)^7f^{(6)}(\xi)	\\
  \hline
  \end{tabular}
\end{center}

Man beachte, dass der Exaktheitsgrad der Newton-Cotes-Formeln immer ungerade ist, d.h. entweder $m$ oder $m+1$.

Mit der Integralaufteilung, die in der Kapiteleinleitung beschrieben wurde, erhalten wir die \emph{summierten Newton-Cotes-Formeln}. Jetzt ist $h = \frac{b - a}{n}$.
\[
  \int_a^b f(x) dx \quad\approx\quad \sum_{k=1}^n h \sum_{j=0}^m c_j f(a + (k-1) h + \xi_j h)
\]

\subsection{Gauß-Quadratur}

Anstatt äquidistante Stützstellen zu wählen, können wir auch andere Stützstellen verwenden. Diese Verallgemeinerung, die Gauß-Quadratur, der Newton-Cotes-Formeln erreicht dann einen Exaktheitsgrad von bis zu $2m + 1$.

Genauer können wir sagen: Es existieren Stützstellen $\omega_0, \ldots, \omega_m \in (c, d)$ (hier im \emph{offenen} Intervall!), so dass mit $h = d - c$ gilt

\[
  h \sum_{i=0}^m \omega_i f(x_i) = \int_c^d f(x) dx + E_f(x)
\]

Dabei ist $E_g(h)$ der Fehler, welcher bei allen $g \in \Pi_{2m+1}$ gleich null ist. Sonst gilt für passendes $\xi \in [c, d]$

\[
  \abs*{E_g(h)} = \frac{((m+1)!)^4}{((2m+2)!)^3 (2m+3)} h^{2m+3} \abs*{g^{(2m+3)}}
\]

Wir schreiben auch auch $I_{k,n} \approx \int_a^b f(x) dx$, wobei $[a, b]$ in $n$ Teilintervalle mit Länge $\frac{b-a}{n} = h$ und auf jedem Teilintervall eine Gauß-Quadratur mit $k = m + 1$ Stützstellen angewandt wird.

Jetzt stellt sich die Frage: Mehr oder weniger Teilintervalle für größere Präzision? Für $I_{2k,n}$ und $I_{k,2n}$ wird die Anzahl der Funktionsauswertungen verdoppelt im Vergleich zu $I_{k,n}$. Außerdem kann man herausfinden, dass $\abs{I - I_{2k,n}} \ll \abs{I - I_{k,2n}}$. Also wird in der Praxis oft $n$ klein gewählt, oft auch $n = 1$.

\section{Lineare Gleichungssysteme: Iterative Lösungsverfahren}

Die Lösung eines linearen Gleichungssystems lässt sich iterativ als
\[x^{k+1} = x^k+ C(b-Ax^k)\]
bestimmen, wobei $C$ eine ``gute'' Approximation für $A^{-1}$ ist.

Dieses Verfahren konvergiert, wenn $\rho(I-CA) < 1$ ist. Dabei ist $\rho(A)$ der Spektralradius, der Betrag des betragsmäßig größten Eigenwertes von $A$.

Da $\rho(A) \le \norm{A}$ für jede Vektornorm-induzierte Matrixnorm $\norm{\cdot}$ ist, konvergiert das Verfahren also insbesondere wenn $\norm{A} < 1$ für eine beliebige Matrixnorm $\norm{\cdot}$ ist.

Im Folgenden sei $A = D-L-U$ wobei $D$ nur Einträge auf, $L$ unterhalb und $U$ oberhalb der Diagonalen enthält.

\subsection{Jacobi-Verfahren}

Beim Jacobi-Verfahren ist $C = D^{-1}$.

\subsection{Gauß-Seidel-Verfahren}

Beim Gauß-Seidel-Verfahren ist $C = (D-L)^{-1}$.

\subsection{SOR-Verfahren}

Beim SOR-Verfahren wählt man ein $\omega \in (0,2)$. Dann ist $C = (\omega D - L)^{-1}$.

\end{document}
