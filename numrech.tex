\documentclass[a4paper,parskip=half*,DIV=15,fontsize=11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[english,ngerman]{babel}
\usepackage{braket}
\usepackage{listings}
\usepackage[pdftex,pdfencoding=auto,psdextra]{hyperref}
\usepackage{xcolor}
\usepackage{array}
\usepackage{microtype}
\usepackage{lmodern}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclareMathOperator*\argmin{arg\,min}
\DeclareMathOperator\sign{sign}
\newcommand{\fl}{\mathrm{fl}}
\newcommand{\defeq}{\vcentcolon =}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\eps}{\mathrm{eps}}
\newcommand{\grad}{\nabla}

\lstset{
    mathescape=true,
    basicstyle=\ttfamily
}
\lstset{literate=%
    {Ö}{{\"O}}1
    {Ä}{{\"A}}1
    {Ü}{{\"U}}1
    {ß}{{\ss}}1
    {ü}{{\"u}}1
    {ä}{{\"a}}1
    {ö}{{\"o}}1
    {~}{{\textasciitilde}}1
}

\newcolumntype{C}{>{$}c<{$}}

\makeatletter
\AtBeginDocument{
  \hypersetup{
    pdftitle = {\@title},
    pdfauthor = {\@author}
  }
}
\makeatother

\title{Numerisches Rechnen Panikzettel}
\author{``der Dude'', Luca Oeljeklaus,\\ Tobias Polock, Philipp Schröer, Caspar Zecha}

\begin{document}

\maketitle

\begin{abstract}
Dieser Panikzettel ist über die Vorlesung Numerisches Rechnen. Er basiert auf dem Foliensatz von Prof. Dr. Martin Grepl aus dem Wintersemester 17/18.	\\
Dieser Panikzettel ist Open Source. Wir freuen uns über Anmerkungen und Verbesserungsvorschläge auf \url{https://git.rwth-aachen.de/philipp.schroer/panikzettel}.
\end{abstract}

\tableofcontents

\section{Fehleranalyse: Kondition, Rundungsfehler, Stabilität}

\subsection{Kondition}

Der \emph{absolute Fehler} in Daten $x$ und gestörten Daten $\tilde{x}$ ist $\norm{\Delta_x} := \norm{\tilde{x}-x}$.
Der \emph{relative Fehler} ist $\delta_x := \frac{\norm{\Delta_x}}{\norm{x}}$.

Für eine Funktion $f$ ist der Fehler in den Ausgabedaten $\tilde{y} = f(\tilde{x})$.

Mit \emph{Kondition} ist meist die \emph{relative Kondition} gemeint: $\frac{\delta_y}{\delta_x}$. Diese beschreibt die \emph{Sensitivität} des Problems bei Störungen in den Eingabedaten. Die \emph{absolute Kondition} ist $\frac{\norm{\Delta_y}}{\norm{\Delta_x}}$. Wir sagen, dass ein Problem \emph{gut konditioniert} ist, wenn seine Kondition klein ($\leq 1$) ist.

Als Approximation (über Taylorentwicklung 1.\ Ordnung) an die Kondition benutzen wir die \emph{Konditionszahl}.
\[\kappa_{\text{rel}}(x) := \abs*{f'(x)\frac{x}{f(x)}}\]

\subsection{Gleitpunktdarstellung}

Da die Menge der Reellen Zahlen nicht im Computer darstellbar ist${}^{\textrm{[Citation needed]}}$, approximieren wir diese durch Maschinenzahlen.
\[\mathbb{M}(b,m,r,R) = \Set{ \pm\sum_{i=1}^m d_{i}b^{e-i} | r \le e \le R,\ 0 \le d_i < b }\]

\begin{itemize}
  \item Relative Maschinengenauigkeit: $\eps := \frac{b^{1-m}}{2} = \inf \Set{ \delta > 0 | \fl(1 + \delta) > 1 }$
  \item Die betragsmäßig kleinste Zahl ($\neq 0$): $x_\mathrm{MIN} = b^{r-1}$
  \item Die betragsmäßig größte Zahl: $x_\mathrm{MAX} = (1 - b^{-m}) b^R$
\end{itemize}

Für IEEE Double-precision floating point-Zahlen ist $\eps = 2.2204 \times 10^{-16}$.

Die Abbildung $\fl : \mathbb{R} \to \mathbb{M}(b,m,r,R)$ rundet gegen $+\infty$ bzw.\ $-\infty$. Es gilt $\fl(x) = x (1 + \varepsilon)$ für ein $\abs \varepsilon \leq \eps$.

Für Multiplikation und Division von Maschinenzahlen ist $\kappa_\text{rel} \leq 1$. Diese Operationen sind also immer gut konditioniert. Bei Addition und Subtraktion gilt dagegen $\kappa_\text{rel} \gg 1$, wenn das Ergebnis gegen Null geht.

\subsection{Stabilität}

\emph{Stabile} Algorithmen erzeugen Fehler nur in der Größenordnung des Fehlers, der durch die Kondition des Problems unvermeidbar ist.

Ein \emph{rückwärts stabiles} Verfahren $\tilde{f}$, das eine Funktion $f$ approximiert, gibt die exakte Antwort auf eine nahezu richtige Frage: Für alle $x \in X$ gilt
\[ \tilde{f}(x) = f(\tilde{x}) \text{ für ein } \tilde{x} \text{ mit } \frac{\norm{x-\tilde{x}}}{\norm{x}} = \mathcal{O}(\eps) \]

Dann wissen wir: $\frac{\norm{\tilde{f}(x) - f(x)}}{\norm{f(x)}} = \mathcal{O}(\kappa(x) \eps)$.

\subsection{Taylorentwicklung}

Funktionen lassen sich als die Taylorentwicklung darstellen. Für ein $\xi$ zwischen $x$ und $x_0$ ist
\[f(x) = \sum_{i=0}^{n-1} \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i + \frac{f^{(n)}(\xi)}{n!}(x-x_0)^n\]

Somit lässt sich eine Funktion $f$ durch das sogenannte Taylorpolynom $n$-ten Grades approximieren:
\[p_n(x) = \sum_{i=0}^{n} \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i\]

\section{Lineare Gleichungssysteme, direkte Lösungsverfahren}

\subsection{Kondition und Störungssätze}

Díe \emph{Konditionszahl einer Matrix} ist $\kappa(A)_{\norm{\cdot}} = \norm{A} \norm{A^{-1}}$ bzgl.\ der $\norm{\cdot}$-Norm.

Für ein lineares Gleichungssystem $Ax = b$ mit Störung in $b$ bzw. in $A$ und $b$ gilt:
\begin{align*}
	\frac{\norm{\Delta x}}{\norm{x}} &\leq \kappa(A) \frac{\norm{\Delta b}}{\norm{b}}	\\
    \frac{\norm{\Delta x}}{\norm{x}} &\leq \frac{\kappa(A)}{1-\kappa(A)\frac{\norm{\Delta A}}{\norm{A}}}\left(\frac{\norm{\Delta A}}{\norm{A}}+\frac{\norm{\Delta b}}{\norm{b}}\right)
\end{align*}

\subsection[LR-Zerlegung]{$LR$-Zerlegung (Gauß-Elimination)}

Bei der Gauß-Elimination wird eine Matrix und die zugehörige rechte Seite durch elementare Zeilentransformationen in rechte obere Dreiecksform gebracht, die anschließend durch Rückwärtseinsetzen einfach gelöst werden kann.

Bei der \emph{LR-Zerlegung} werden diese Zeilentransformationen zusätzlich in einer Matrix $L$ gespeichert. So wird $A \in \mathbb{R}^{n \times n}$ in $A = LR$ zerlegt.
$L$ ist eine normierte linke untere und $R$ eine rechte obere Dreiecksmatrix.

Etwa ist $L_{2,1} = \frac{A_{2,1}}{A_{1,1}}$, also der Faktor mit dem man Zeile 1 von Zeile 2 subtrahiert, um $A_{2,1}$ zu eliminieren.

So können wir $Ax = b$ über $L y = b$ und $Rx = y$ lösen.

Um die Kondition des Problems zu verbessern, können wir mit Diagonalmatrix $D_Z$ \emph{äquilibieren}. Wir teilen jede Zeile durch ihre Betragssumme. Das gibt uns die minimale Konditionszahl bezüglich der $\infty$-Norm.

Die \emph{LR-Zerlegung mit Spaltenpivotisierung} ist eine Zerlegung $PA = LR$ mit einer Permutationsmatrix $P$. $P$ entsteht, indem in jeder Spalte das betragsgrößte Element als \emph{Pivotelement} wählt und dafür die Matrixzeilen vertauscht. Dies ist nötig, da sonst das Pivotelement verschwinden könnte (Division durch null!) und es ist sowieso gut für Stabilität.

\subsection[Cholesky-Zerlegung]{Cholesky-Zerlegung ($LDL^T$-Zerlegung)}

\begin{minipage}[t]{0.5\textwidth}
Für symmetrisch positiv definite Matrix $A \in \mathbb{R}^{n \times n}$ existiert eine Zerlegung
\[A = LDL^T\]
wobei $L$ eine normierte linke untere Dreiecksmatrix und $D$ eine Diagonalmatrix ist.
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vspace{-2em}
\begin{align*}
  d_{k,k} &= a_{k,k} - \sum_{j=1}^{k-1} l_{k,j}^2 d_{j,j} \\
  l_{i,k} &= \left( a_{i,k} - \sum_{j=1}^{k-1} l_{i,j} d_{j,j} l_{k,j} \right) / d_{k,k}
\end{align*}
\end{minipage}

Zur Berechnung läuft man einfach durch die Spalten und berechnet $d_{1,1}, l_{2,1}, \ldots$. Auch cool: Ist $d_{i,i} > 0$ für alle $i$ und $L$ normierte Dreiecksmatrix, dann ist $A$ symmetrisch positiv definit! Cholesky kann also erst angewandt werden und dann das Ergebnis auf Gültigkeit geprüft werden.

Für $n = 3$ ergibt sich beispielsweise
\begin{align*}
LDL^T &= \begin{pmatrix}
	1	\\
    l_{2,1}	&	1	\\
    l_{3,1}	&	l_{3,2}	&	1
\end{pmatrix} \begin{pmatrix}
	d_{1,1}	\\
    	&	d_{2,2}	\\
        &	&	d_{3,3}
\end{pmatrix} \begin{pmatrix}
	1	&	l_{2,1}	&	l_{3,1}	\\
    	&	1	&	l_{3,2}	\\
    	&	&	1
\end{pmatrix}	\\
&= \begin{pmatrix}
	d_{1,1}	&	l_{2,1}d_{1,1}	&	l_{3,1}d_{1,1}	\\
    l_{2,1}d_{1,1}	&	l_{2,1}^2d_{1,1} + d_{2,2}	&	l_{2,1}l_{3,1}d_{1,1}+l_{3,2}d_{2,2}	\\
    l_{3,1}d_{1,1}	&	l_{2,1}l_{3,1}d_{1,1}+l_{3,2}d_{2,2}	&	l_{3,1}^2d_{1,1} + l_{3,2}^2d_{2,2} + d_{3,3}
\end{pmatrix}
\end{align*}

\subsection[QR-Zerlegung]{$QR$-Zerlegung}

Bei der $QR$-Zerlegung wird eine Matrix $A$ durch orthogonale Transformationen $Q^T$ in rechte obere Dreiecksform gebracht. Rechte obere Dreiecksform bedeutet, dass die ersten $k$ Spalten von $R$ den selben Raum wie die ersten $k$ Einheitsvektoren aufspannen. Es geht also bei der $QR$-Zerlegung darum, durch orthogonale Transformationen den Raum, der von den ersten $k$ Spalten von $A$ aufgespannt wird, in die von den ersten $k$ Einheitvektoren aufgespannte Hyperebene abzubilden.

Da orthogonale Transformationen genau Kombinationen von Rotationen und Spiegelungen sind, ergeben sich folgende drei Optionen für die Transformationen.

\subsubsection{Gram-Schmidt-Orthogonalisierung}

Ausführlich erklärt in unserem \href{http://panikzettel.philworld.de/la.pdf#page=20}{Panikzettel zu Lineare Algebra}. Dieses Verfahren ist nicht stabil.

\subsubsection{Givens-Rotation}

Die Givens-Rotationsmatrix $G_{i,k}$ rotiert in der $x_i$-$x_k$-Ebene die Matrix $A$ so, dass in $G_{i,k} A$ der Eintrag an Position $i, k$ Null wird.

$G_{i,i} = c = G_{k,k}$, $G_{i,k} = s$ und $G_{k,i} = -s$. Dazu Einsen auf die Diagonale und sonst Nullen.
\begin{minipage}{0.7\textwidth}
\[
  G_{i,k} = \left(\begin{array}{ccccccccccc}
    1 \\
    & \ddots  & & & & & & & & 0 \\
    & & 1 \\
    & & & c & 0 & \cdots  & 0 & s \\
    & & & 0 & 1 & & & 0 \\
    & & & \vdots  & & \ddots  & & \vdots  \\
    & & & 0 & & & 1 & 0 \\
    & & & -s  & 0 & \cdots  & 0 & c \\
    & & & & & & & & 1 \\
    & 0 & & & & & & & & \ddots  \\
    & & & & & & & & & & 1
  \end{array}\right)
\]
\end{minipage}
\begin{minipage}{0.3\textwidth}
\begin{align*}
   r &= \pm\norm*{\left(\begin{array}{c}
   A_{i,i}  \\  A_{i,k}
   \end{array}\right)}_2  \\
   c &= \frac{A_{i,i}}{r} \\
   s &= \frac{A_{i,k}}{r}
\end{align*}
\end{minipage}

Wir wenden also diese Rotationsmatrizen auf alle Elemente unterhalb der Diagonalen von $A$ an, sodass wir eine QR-Zerlegung für $A$ erhalten:
\[ R = \underbrace{\ldots G_{2,3} \ldots G_{1,2}}_{Q^T} A \text{ bzw. }  A = \underbrace{G_{1,2}^T \ldots G_{2,3}^T \ldots}_{Q} R \]

$Q$ wird nie explizit berechnet. Zur Lösung von $QRx = b$: Berechne $y = \ldots G_{2,3} \ldots G_{1,2} b$ und löse $Rx = y$ durch Rückwärtseinsetzen.

\subsubsection{Householder-Transformation}

Die Householder-Transformation entspricht Spiegelungen. Die Householder-Matrix $Q_v$ spiegelt an der zu $v$ orthogonalen Hyperebene ($v$ ist Normalenvektor).
\[ Q_v = I - 2 \frac{v v^T}{v^T v} \]

$v v^T$ ist eine Dyade. $v^T v$ ist ein Skalar zur richtigen Skalierung. Die 2 taucht auf, weil man ja nicht genau einmal den Normalenvektor in die Hyperebene, sondern genau zwei mal den Normalenvektor durch (auf die andere Seite) der Hyperebene will.

Die folgenden Eigenschaften sind schön zu beweisen:

\begin{minipage}[t]{0.5\textwidth}
\begin{itemize}
  \item Orthogonalität: $Q_v^{-1} = Q_v^T$.
  \item Selbstinverse: $Q_v^2 = I$.\\Zweimalige Spiegelung ergibt den ursprünglichen Punkt.
  \item $Q_{\alpha v} = Q_v, \alpha \in \mathbb{R} \setminus \{0\}$.\\Skalierung des Normalenvektors ändert nicht die Spiegelebene.
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{itemize}
  \item $Q_v y = y \Leftrightarrow y^T v = 0$.\\
        Ursprünglicher und gespiegelter Punkt sind nur identisch, wenn der Punkt in der Spiegelebene liegt.
  \item $Q_v v = -v$.\\
        Spiegelung des Normalenvektors vertauscht das Vorzeichen.
\end{itemize}
\end{minipage}

Mit Householder schauen wir uns immer die erste Spalte $a_1$ an und spiegeln die auf die Ebene zum Einheitsvektor $e_1$. Danach wiederholen wir das Gleiche auf der Teilmatrix $A$ ohne erste Zeile und Spalte (in den $Q_v$ fügen wir passen Nullen ein).
\[v = a_1 + \sign(a_{1,1}) \norm{a_1} e\]

Mit $Q_i$ als $Q_v$ mit $v$ für Teilmatrix $i$ ist dann die QR-Zerlegung:
\[Q_{n-1} \ldots Q_2 Q_1 A = R \text{ bzw. } A = Q_1^T Q_2^T \ldots Q_{n-1}^T R = QR\]

Übrigens ist allgemeiner $v = a_1 \pm \norm{a_1} e_1$ möglich, aber wir möchten Auslöschung vermeiden.

\section{Lineare Ausgleichsrechnung}

Bei einem \emph{Linearen Ausgleichsproblem} wollen wir ein überbestimmtes LGS so lösen, dass der Fehler möglichst klein ist. Für $A \in \mathbb{R}^{n \times m}$ und $b \in \mathbb{R}^n$ mit $m < n$ suchen wir $x^\ast$:
\[x^\ast = \argmin_{x \in \mathbb{R}^{m}} \norm*{Ax-b}\]

Im Folgenden werden wir annehmen, dass es sich bei der gewählten Norm um die $2$-Norm handelt, da das Problem sonst nicht eindeutig lösbar ist.

Geometrische Vorstellung: Wir wollen $x^\ast$ so, dass das Residuum $r = b - Ax^\ast$ orthogonal auf $Ax^\ast$ steht, denn dann wird $r$ am kleinsten.

\subsection{Normalengleichungen}

Mit der Lösung von \emph{Normalengleichungen} lässt sich das Problem auf einfache LGS zurückführen:
\[\left(A^TA\right)x^\ast = A^Tb\]

Es gibt immer eine Lösung. Sie ist genau dann eindeutig, wenn $A$ vollen Rang hat.

Ausgleichsrechnung über Normalengleichungen ist allerdings numerisch instabil.

\subsection[QR-Zerlegung]{$QR$-Zerlegung}

Ein numerisch stabiles Verfahren zur Lösung des Linearen Ausgleichsproblems ergibt sich mithilfe der $QR$-Zerlegung.

Sei eine QR-Zerlegung gegeben mit $Q \in \mathbb{R}^{n \times n}$ und $R = \begin{pmatrix} \hat{R} \\ 0 \end{pmatrix} \in \mathbb{R}^{n \times m}$, wobei $\hat{R} \in \mathbb{R}^{m \times m}$. Weil orthogonale Projektionen die 2-Norm nicht ändern, gilt:
\[
  x^\ast = \argmin_{x \in \mathbb{R}^{m}} \norm*{Ax-b}_2 = \argmin_{x \in \mathbb{R}^m} \norm*{Rx-Q^Tb}_2
\]

Berechnen wir $\hat{b} \in \mathbb{R}^n$ mit $Q^T b = \begin{pmatrix} \hat{b} \\ r \end{pmatrix}$, können wir $\hat{R}x^\ast = \hat{b}$  durch Rückwärtseinsetzen lösen. Dann ist auch die Norm des Residuums durch $\norm{r}_2$ gegeben.

\subsection{Singulärwertzerlegung (SVD)}

Die \emph{Singulärwertzerlegung} einer Matrix $A \in \mathbb{R}^{n \times m}$ ist
\[A = U\Sigma V^T\]
mit orthogonalen Matrizen $U \in \mathbb{R}^{n \times n}, V \in \mathbb{R}^{m \times m}$ und einer Diagonalmatrix $\Sigma \in \mathbb{R}^{n \times m}$. Die nicht-null Einträge von $\Sigma$ heißen Singulärwerte von $A$, die Spalten von $U$ heißen Linkssingulärvektoren, die Spalten von $V$ heißen Rechstsingulärvektoren.

SVD gibt die Pseudoinverse $A^+ := V\Sigma^+ U^T$ mit $\Sigma^+$ der Diagonalmatrix, die die invertierten Singulärwerte von $A$ enthält.

Auch damit können wir das Lineare Ausgleichsproblem stabil lösen:
\[ x^\ast = A^+ b \implies x^\ast = \argmin_{x \in \mathbb{R}^m} \norm*{Ax-b}\]
Sind die Singulärvektoren nach betragsmäßiger Größe geordnet, ergibt sich die Lösung mit kleinster $2$-Norm.

\section{Nichtlineare Gleichungssysteme}

\subsection{Fixpunktiteration}

Für eine Funktion $\Phi: \mathbb{R}^n \to \mathbb{R}^n$  heißt $x \in \mathbb{R}^n$ Fixpunkt von $\Phi$ genau dann wenn
\[\Phi(x) = x\]
gilt.

Es sei eine abgeschlossene nichtleere Teilmenge $M \subseteq \mathbb{R}^n$, auf der $\Phi$ eine Selbstabbildung und Kontraktion ist, d.h. $\Phi(M) \subseteq M$ und es gibt ein $\theta < 1$ sodass für alle $x,y \in M$, $x \ne y$ $\norm{\Phi(x)-\Phi(y)} \le \theta \norm{x-y}$ ist.	\\
Dann hat $\Phi$ einen eindeutigen Fixpunkt $x^\ast \in M$ und die Fixpunktiteration
\[x_{i+1} = \Phi(x_i)\]
konvergiert für alle $x_0 \in M$ mit
\begin{align*}
	\norm{x_k-x^\ast} &\le \frac{\theta^k}{1-\theta}\norm{x_1-x_0}	&&	\text{(a priori)}	\\
	\norm{x_k-x^\ast} &\le \frac{\theta}{1-\theta} \norm{x_k-x_{k-1}}	&&	\text{(a posteriori)}
\end{align*}

\subsection{Newton-Verfahren}

Es sei $F : \mathbb{R}^n \to \mathbb{R}^n$ stetig differenzierbar. Dann ist
\[x_{k+1} = x_k - F'(x_k)^{-1}F(x_k)\]
die Newton-Iteration zum Startwert $x_0$. Diese konvergiert lokal quadratisch.

\section{Nichtlineare Ausgleichsrechnung}

Beim nichtlinearen Ausgleichsproblem such man für eine Funktion $F : \mathbb{R}^n \to \mathbb{R}^m$ ein $x^\ast$ sodass
\[x^\ast = \argmin_{x \in \mathbb{R}^n} \norm{F(x)}_2\]

Dies Formulieren wir um als Suche nach dem Minimum von
\[\phi(x) : \mathbb{R}^n \to \mathbb{R}, x \mapsto \frac{1}{2} \norm{F(x)}_2^2 \]

Diese hat an einer Stelle $x^\ast$ ein lokales Minimum, wenn
\begin{align*}
	\grad\phi(x^\ast) &= 0	\\
    \phi''(x^\ast) &~\text{s.p.d.}
\end{align*}

\subsection{Gauß-Newton-Verfahren}

Im Gauss-Newton-Verfahren bestimmen wir diese Nullstelle von $\grad\phi(x^\ast) = 0$ über eine Newton-Iteration. Allerdings ersetzen wir dabei in jedem Schritt die Funktion $F$ durch eine lineare Approximation mithilfe der Taylorentwicklung. Die Iteration geht also wie folgt, hier als Matlab/Octave-Code:

\begin{lstlisting}[language=octave]
function [x_star] = gauss_newton(f, df, x0, tolerance, maxIterations):
    x = x0;
    iterations = 0;
    fx = eval(f,x);
    dfx = eval(df,x);
    while (norm(dfx' * fx) > tolerance && iterations < maxIterations):
        step = least_squares(dfx,-fx);
        x = x + step;
        fx = eval(f,x);
        dfx = eval(df,x);
        iterations = iterations + 1;
    end
    x_star = x;
end
\end{lstlisting}

wobei \lstinline[language=octave]{least_squares(A,b)} das Lineare Ausgleichsproblem
\[x^\ast = \argmin_{x \in \mathbb{R}^n} \norm{Ax-b}_2\]
löst.

\subsection{Levenberg-Marquardt-Verfahren}

Das Levenberg-Marquardt-Verfahren ist eine Modifikation des Gauß-Verfahrens bei dem bei der Berechnung der Schrittweite ein Dämpfungsfaktor $\mu$ eingefügt wird. Statt des lineare Ausgleichsproblems
\[s^k = \argmin_{s \in \mathbb{R}^n} \norm*{DF(x^k)s + F(x^k)}\]
lösen wir also das Problem
\[s^k = \argmin_{s \in \mathbb{R}^n} \norm*{\left(\begin{array}{c}
    F'(x^k)	\\	\mu I
\end{array}\right)s + \left(\begin{array}{c}
    F(x^k)	\\	0
\end{array}\right)}\]

\section{Interpolation}
Den Raum der Polynome vom Grad $n$ bezeichnen wir als $\Pi_n$, und deren Elemente als $P_n (x)$. Zudem haben wir Daten (z.B. Messwerte) $(x_i,f(x_i) = y_i),\, i \in \{0,\ldots,n\}$ gegeben, und wollen diese durch ein Polynom $P_n (f|x_0,\ldots, x_n) = P_n (x)$ darstellen, es soll also $P_n (x_i) = y_i$ sein. Ein solches Polynom existiert immer eindeutig.

\subsection{Lagrange-Fundamentalpolynome}
Die Lagrange-Fundamentalpolynome (erfüllen unsere Interpolationsbedingung), sind gegeben durch:

\[\ell_i(x) = \prod_{\substack{k = 0 \\ k \neq i}}^n \frac{x - x_k}{x_i - x_k}\]

Diese bilden eine Orthogonalbasis von $\Pi_n$.

Dann ist das Interpolationspolynom

\[P_n (x) = \sum_{j=0}^n f (x_j) \ell_j\]

\subsection{Auswertung des Polynoms durch das Neville-Aitken Schema}
Wir wollen nun den Wert unseres Interpolationspolynoms an der festen Stelle $x$ durch ein rekursives Schema auswerten. Dazu setzen wir:

\[P_{i,k} = P(f|x_{i-k},\ldots,x_i)\]
also
\[P_{i,0} = P(f|x_i)(x) = f(x_i)\]
und
\[P_{n,n} = P(f|x_0,\ldots,x_n)\]

Insbesondere wollen wir also $P_{n,n}$ bestimmen. Das Schema lässt sich dann rekursiv berechnen durch:

\[P_{i,k} = P_{i,k-1} + \frac{x - x_i}{x_i - x_{i-k}} (P_{i,k-1} - P_{i-1,k-1})\]

Die Rekursionsendpunkte sind dann $P_{i,0}$.

\subsection{Berechnung der Potenzform über ein LGS}
Wir können das Interpolationspolynom $P_n(x)$ auch über das folgende Gleichungssystem bestimmen:

\[\underbrace{\begin{pmatrix} 1 & x_0 & x_0^2 & \cdots & x_0^n \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \ldots & x_n^n\end{pmatrix}}_{\text{Vandermonde-Matrix $V_n$}} \cdot \begin{pmatrix} a_0 \\ \vdots \\ a_n \end{pmatrix} = \begin{pmatrix} y_0 \\ \vdots \\ y_n \end{pmatrix}\]

Jedoch hat dies den gravierenden Nachteil, dass die Vandermonde-Matrix im Allgemeinen sehr schlecht konditioniert ist.

\subsection{Auswertung der Potenzform mittels Horner-Schema}
Falls die Potenzform des Polynoms $P_n (x)$ vorliegt, kann diese durch geschickte Klammerung sehr schnell für ein $x$ ausgewertet werden.

\[a_0 + a_1x + \hdots + a_nx^n = a_0 + x\cdot (a_1 + x\cdot (a_2 + (\hdots  x\cdot (a_{n_1} + xa_n) \hdots) ) )\]

\subsection{Newtonsche Interpolationsformel}
Sei $P(f|x_0,\ldots,x_{n-1})$ gegeben und wir wollen nun mit möglichst wenig Aufwand $P(f|x_0,\ldots,x_n)$ berechnen. Dies können wir wie folgt erreichen:

\[P(f|x_0,\ldots,x_n)(x) = P(f|x_0,\ldots,x_{n-1}) (x) + \delta_n(x-x_0)\ldots (x-x_{n-1})\]

mit

\[\delta_n = \frac{f(x_n) - P(f|x_0,\ldots,x_{n-1})(x_n)}{(x_n -x_0)\ldots (x_n-x_{n-1})} \eqdef [x_0,\ldots,x_n]f\]

Mit dieser sogenannten Newtonschen Interpolationsformel lässt sich rekursiv beginnend bei einer Stützstelle (z.B. $P(f|x_0)$) das komplette Polynom $P(f|x_0,\ldots,x_n)$ bestimmen. Als Knotenpolynome $\omega_i (x)$ bezeichnen wir die hierbei entstehende Polynome:

\begin{eqnarray}
\omega_0 &\defeq& 1 \nonumber \\
\omega_1 &\defeq& (x-x_0) \nonumber \\
&\vdots& \nonumber \\
\omega_n &\defeq& (x-x_0)\ldots (x-x_{n-1}) \nonumber
\end{eqnarray}

Diese bilden die Newton-Basis von $\Pi_n$.

Mit der Rekursionsformel von Aitken lässt sich zudem zeigen, dass für verschiedene Stützstellen $x_0,\ldots,x_n$ folgendes gilt:

\[ [x_0,\ldots,x_n]f = \frac{[x_1,\ldots, x_n]f - [x_0,\ldots, x_{n-1}]f}{x_n - x_0} \]

Diese Terme werden auch als dividierte Differenzen bezeichnet.

\subsection{Fehlerabschätzung}
Der Fehler der Polynominterpolation $P_n (f|x_0,\ldots,x_n) (x)$ zur interpolierten Funktion $f(x)$, auf dem Intervall $[a,b]$, kann abgeschätzt werden durch:

\[\abs*{f(x) - P_n (x)} \leq \abs*{\omega_{n+1}} \max_{\xi \in [a,b]} \frac{\abs*{f^{(n+1)}(\xi)}}{(n+1)!}\]

\section{Numerische Differentiation}

Der nummerischen Differentiation liegt folgender Gedanke zugrunde:

\[P(f|x_0, \ldots, x_n)^{(n)} (x) = n![x_0,\ldots,x_n]f \approx f^{(n)} (x)\]

\subsection{Complex Step Differentiation}

Für analytische $f: \mathbb{C} \to \mathbb{C}$ mit $f(\mathbb{R}) \subseteq \mathbb{R}$ und $x \in \mathbb{R}$ ist

\[ f'(x) = \lim_{h \to 0} \frac{f(x+ih)}{h} \]

Dies kann durch kleine $h$ angenähert werden. Dabei wird Auslöschung durch Wahl von zu kleinen $h$ vermieden.\footnote{Martins, Joaquim RRA, Peter Sturdza, and Juan J. Alonso. ``The complex-step derivative approximation.'' ACM Transactions on Mathematical Software (TOMS) 29.3 (2003): 245-262.}

\section{Numerische Integration}

Der Ansatz bei der numerischen Integration ist der Folgende: Das Integral $\int_a^b f(x) dx$ wird in Teilstücke aufgeteilt und dann werden diese Teilintegrale mithilfe einer einfach integrierbaren Funktion approximiert.

Formal teilen wir also das Intervall $[a, b]$ in Teilintervalle $[t_{k-1}, t_k]$ auf und approximieren wie folgt:

\[
	\int_a^b f(x) dx = \sum_{k=1}^n \int_{t_{k-1}}^{t_k} f(x) dx \approx \sum_{k=1}^n \int_{t_{k-1}}^{t_k} g_k(x)dx
\]

\subsection{Kondition}

Betrachten wir nun die Kondition des Approximierens von Integranden. Wenn wir $f$ durch $\tilde{f}$ approximieren:

\begin{align*}
	I = \int_a^b f(x) dx & & \tilde{I} = \int_a^b \tilde{f}(x) dx
\end{align*}

Dann gilt: $\abs{I - \tilde{I}} \leq (b - a) \norm{f - \tilde{f}}_\infty$. Also ist die Kondition:

\[
  \frac{\abs{I - \tilde{I}}}{\abs{I}} \leq \underbrace{\frac{\int_a^b \norm{f}_\infty dx}{\norm{\int_a^b f(x) dx}}}_{\kappa_\mathrm{rel}} \cdot \frac{\norm{f - \tilde{f}}_\infty}{\norm{f}_\infty}
\]

\subsection{Quadraturformeln}

Wenn über einem Intervall $[c, d]$ mithilfe der Punkte $x_0, \ldots, x_m \in [c, d]$ die Funktion $f$ approximiert wird, ergibt sich eine sogenannte \emph{Quadraturformel}:

\[
  I_m(f) = \int_c^d P(f \vert x_0, \ldots, x_m)(x) dx
\]

Diese Quadraturformel ist für Polynome mit bis zu Grad $m$ exakt. Formal: Für $g \in \Pi_m$ gilt $I_m(g) = \int_c^d g(x) dx$.

\subsection{Newton-Cotes-Formeln}

Wir können die Quadraturformel mithilfe der Lagrange-Fundamentalpolynome $c_j$ auch folgendermaßen schreiben, wobei $h = d - c$:

\[
  I_m(f) = h \sum_{j=0}^m c_j f(x_j)
\]

Die Lagrange-Fundamentalpolynome $\ell_{jm}$ sind nur von den Stützstellen, aber \emph{nicht von den Funktionswerten} abhängig.

\[
  c_j = \frac{1}{h} \int_c^d \prod_{\substack{k=0 \\ k\neq j}}^m \frac{x - x_k}{x_j - x_k} dx = \frac{1}{h} \int_c^d \ell_{jm}(x) dx
\]

Man erhält dann die Newton-Cotes-Formeln mit \emph{normierten} Stützstellen $\xi_j$ und Gewichten $c_j$. Diese sind unabhängig vom Intervall $[c, d]$.

\[
  I_m(f) = h \sum_{j=0}^m c_j f(c + \xi_j h)
\]

Mit äquidistanten Stützstellen:

\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|C|c|C|C|C|}
\hline
m	&	&	\xi_j	&	c_j	&	I_m(f) - \int_c^d f(x) dx	\\\hline
0	&	Mittelpunktsregel	&	\frac{1}{2}	&	1	&	-\frac{1}{24}h^3 f^{(2)}(\xi)	\\
1	&	Trapezregel	&	0, 1	&	\frac{1}{2}, \frac{1}{2}	&	\frac{1}{12}h^3 f^{(2)}(\xi)	\\
2	&	Simpson-Regel	&	0, \frac{1}{2}, 1	&	\frac{1}{6}, \frac{4}{6}, \frac{1}{6}	&	\frac{1}{90} \left ( \frac{1}{2} h \right)^5 f^{(4)}(\xi)	\\
3	&	$\frac{3}{8}$-Regel	&	0, \frac{1}{3}, \frac{2}{3},~1	&	\frac{1}{8}, \frac{3}{8}, \frac{3}{8}, \frac{1}{8}	&	\frac{3}{80} \left( \frac{1}{3} h \right)^5 f^{(4)}(\xi)	\\
4	&	Milne-Regel	&	0, \frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1	&	\frac{7}{90}, \frac{32}{90}, \frac{12}{90}, \frac{32}{90}, \frac{7}{90}	&	\frac{8}{945}\left(\frac{1}{4}h\right)^7f^{(6)}(\xi)	\\
\hline
\end{tabular}\\

Der Exaktheitsgrad der Newton-Cotes-Formeln ist also immer ungerade, d.h. entweder $m$ oder $m+1$.

Nun lässt sich, wie in der Kapiteleinleitung beschrieben, die Integralaufteilung zur Approximation durchführen. Damit kommen wir zu den summierten Newton-Cotes-Formeln. Mit $h = \frac{b - a}{n}$:

\[
  \int_a^b f(x) dx \approx \sum_{k=1}^n h \sum_{j=0}^m c_j f(a + (k-1) h + \xi_j h)
\]

\subsection{Gauß-Quadratur}

Anstatt äquidistante Stützstellen zu wählen, können wir auch andere Stützstellen verwenden. Diese Verallgemeinerung, die Gauß-Quadratur, der Newton-Cotes-Formeln erreicht dann einen Exaktheitsgrad von bis zu $2m + 1$.

Genauer können wir sagen: Es existieren Stützstellen $\omega_0, \ldots, \omega_m \in (c, d)$ (hier im \emph{offenen} Intervall!), so dass mit $h = d - c$ gilt

\[
  h \sum_{i=0}^m \omega_i f(x_i) = \int_c^d f(x) dx + E_f(x)
\]

Dabei ist $E_g(h)$ der Fehler, welcher bei allen $g \in \Pi_{2m+1}$ gleich null ist. Sonst gilt für passendes $\xi \in [c, d]$

\[
  \abs*{E_g(h)} = \frac{((m+1)!)^4}{((2m+2)!)^3 (2m+3)} h^{2m+3} \abs*{g^{(2m+3)}}
\]

Wir schreiben auch auch $I_{k,n} \approx \int_a^b f(x) dx$, wobei $[a, b]$ in $n$ Teilintervalle mit Länge $\frac{b-a}{n} = h$ und auf jedem Teilintervall eine Gauß-Quadratur mit $k = m + 1$ Stützstellen angewandt wird.

Jetzt stellt sich die Frage: Mehr oder weniger Teilintervalle für größere Präzision? Für $I_{2k,n}$ und $I_{k,2n}$ wird die Anzahl der Funktionsauswertungen verdoppelt im Vergleich zu $I_{k,n}$. Außerdem kann man herausfinden, dass $\abs{I - I_{2k,n}} \ll \abs{I - I_{k,2n}}$. Also wird in der Praxis oft $n$ klein gewählt, oft auch $n = 1$.

\section{Lineare Gleichungssysteme: Iterative Lösungsverfahren}

Die Lösung eines linearen Gleichungssystems lässt sich iterativ als
\[x^{k+1} = x^k+ C(b-Ax^k)\]
bestimmen, wobei $C$ eine ``gute'' Approximation für $A^{-1}$ ist.

Dieses Verfahren konvergiert, wenn $\rho(I-CA) < 1$ ist. Dabei ist $\rho(A)$ der Spektralradius, der Betrag des betragsmäßig größten Eigenwertes von $A$.

Da $\rho(A) \le \norm{A}$ für jede Vektornorm-induzierte Matrixnorm $\norm{\cdot}$ ist, konvergiert das Verfahren also insbesondere wenn $\norm{A} < 1$ für eine beliebige Matrixnorm $\norm{\cdot}$ ist.

Im Folgenden sei $A = D-L-U$ wobei $D$ nur Einträge auf, $L$ unterhalb und $U$ oberhalb der Diagonalen enthält.

\subsection{Jacobi-Verfahren}

Beim Jacobi-Verfahren ist $C = D^{-1}$.

\subsection{Gauß-Seidel-Verfahren}

Beim Gauß-Seidel-Verfahren ist $C = (D-L)^{-1}$.

\subsection{SOR-Verfahren}

Beim SOR-Verfahren wählt man ein $\omega \in (0,2)$. Dann ist $C = (\omega D - L)^{-1}$.

\end{document}
